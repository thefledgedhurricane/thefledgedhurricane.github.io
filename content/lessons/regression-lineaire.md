---
title: "R√©gression lin√©aire et logistique"
description: "Ma√Ætrisez les fondamentaux de la r√©gression pour la pr√©diction et la classification"
difficulty: "intermediate"
estimatedTime: "40 minutes"
keywords: ["r√©gression lin√©aire", "r√©gression logistique", "moindres carr√©s", "gradient descent"]
---

# R√©gression lin√©aire et logistique

## üéØ Objectifs d'apprentissage

√Ä la fin de cette le√ßon, vous serez capable de :
- ‚úÖ Comprendre les principes de la r√©gression lin√©aire
- ‚úÖ Impl√©menter la r√©gression logistique pour la classification
- ‚úÖ Optimiser les param√®tres avec la descente de gradient
- ‚úÖ √âvaluer et interpr√©ter les r√©sultats

---

## üìà R√©gression lin√©aire

### Principe fondamental

La **r√©gression lin√©aire** mod√©lise la relation entre une variable d√©pendante et des variables ind√©pendantes par une fonction lin√©aire.

```mermaid
graph LR
    subgraph "R√©gression simple"
        A[x: Variable ind√©pendante] --> B[f(x) = ax + b]
        B --> C[y: Variable d√©pendante]
    end
    
    subgraph "R√©gression multiple"
        D[x‚ÇÅ, x‚ÇÇ, ..., x‚Çô] --> E[f(x) = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b]
        E --> F[y: Pr√©diction]
    end
```

### Formulation math√©matique

#### R√©gression simple
**√âquation** : ≈∑ = ax + b

o√π :
- **≈∑** : valeur pr√©dite
- **a** : pente (coefficient)
- **b** : ordonn√©e √† l'origine (biais)
- **x** : variable d'entr√©e

#### R√©gression multiple
**√âquation** : ≈∑ = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b

**Notation vectorielle** : ≈∑ = w^T x + b

### Visualisation g√©om√©trique

```mermaid
graph TD
    subgraph "Espace 2D (r√©gression simple)"
        A[Points de donn√©es] --> B[Droite de r√©gression]
        B --> C[Minimise l'erreur]
    end
    
    subgraph "Espace 3D+ (r√©gression multiple)"
        D[Points dans l'espace] --> E[Hyperplan de r√©gression]
        E --> F[Projection optimale]
    end
```

### Fonction de co√ªt : Moindres carr√©s

**Objectif** : Minimiser l'erreur quadratique moyenne

MSE = (1/n) √ó Œ£·µ¢(y·µ¢ - ≈∑·µ¢)¬≤

```mermaid
graph TD
    A[Donn√©es observ√©es: y] --> C[Calcul erreur]
    B[Pr√©dictions: ≈∑] --> C
    C --> D[Erreur quadratique]
    D --> E[Moyenne sur tous les points]
    E --> F[MSE √† minimiser]
    
    style D fill:#ffcdd2
    style F fill:#c8e6c9
```

### Solution analytique

Pour la r√©gression lin√©aire simple :

**Formules optimales** :
- a = Œ£[(x·µ¢ - xÃÑ)(y·µ¢ - »≥)] / Œ£[(x·µ¢ - xÃÑ)¬≤]
- b = »≥ - a √ó xÃÑ

**En notation matricielle** :
w = (X^T X)‚Åª¬π X^T y

### Impl√©mentation Python

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# G√©n√©ration de donn√©es exemple
np.random.seed(42)
X = np.random.randn(100, 1)
y = 3 * X.squeeze() + 2 + np.random.randn(100) * 0.5

# Entra√Ænement du mod√®le
model = LinearRegression()
model.fit(X, y)

# Pr√©dictions
y_pred = model.predict(X)

# √âvaluation
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Coefficient: {model.coef_[0]:.2f}")
print(f"Intercept: {model.intercept_:.2f}")
print(f"MSE: {mse:.2f}")
print(f"R¬≤: {r2:.2f}")
```

---

## üéØ R√©gression logistique

### Du lin√©aire au logistique

**Probl√®me** : La r√©gression lin√©aire produit des valeurs continues, mais la classification n√©cessite des probabilit√©s (0-1).

**Solution** : Fonction sigmo√Øde pour "compresser" la sortie lin√©aire.

```mermaid
graph LR
    A[Entr√©e lin√©aire<br/>z = wx + b] --> B[Fonction sigmo√Øde<br/>œÉ(z) = 1/(1 + e^-z)]
    B --> C[Probabilit√©<br/>p ‚àà [0,1]]
    
    subgraph "D√©cision"
        C --> D{p ‚â• 0.5?}
        D -->|Oui| E[Classe 1]
        D -->|Non| F[Classe 0]
    end
```

### Fonction sigmo√Øde

**√âquation** : œÉ(z) = 1 / (1 + e^(-z))

**Propri√©t√©s** :
- Sortie entre 0 et 1 (interpr√©table comme probabilit√©)
- Forme en S lisse
- D√©riv√©e simple : œÉ'(z) = œÉ(z)(1 - œÉ(z))

```mermaid
graph TD
    subgraph "Comportement de la sigmo√Øde"
        A[z ‚Üí -‚àû] --> B[œÉ(z) ‚Üí 0]
        C[z = 0] --> D[œÉ(z) = 0.5]
        E[z ‚Üí +‚àû] --> F[œÉ(z) ‚Üí 1]
    end
```

### Fonction de co√ªt : Log-vraisemblance

**Probl√®me** : MSE n'est pas convexe pour la r√©gression logistique

**Solution** : Log-loss (entropie crois√©e)

**Loss = -[y log(p) + (1-y) log(1-p)]**

```mermaid
graph TD
    A[Vraie classe: y] --> C[Calcul log-loss]
    B[Probabilit√© pr√©dite: p] --> C
    C --> D[P√©nalit√© forte si p loin de y]
    
    subgraph "Exemples"
        E[y=1, p=0.9] --> F[Loss faible ‚úì]
        G[y=1, p=0.1] --> H[Loss √©lev√©e ‚úó]
    end
```

### Optimisation : Descente de gradient

**Principe** : Mise √† jour it√©rative des param√®tres

```mermaid
flowchart TD
    A[Initialiser w, b] --> B[Calculer pr√©dictions]
    B --> C[Calculer loss]
    C --> D[Calculer gradients]
    D --> E[Mettre √† jour param√®tres]
    E --> F{Convergence?}
    F -->|Non| B
    F -->|Oui| G[Mod√®le entra√Æn√©]
    
    style A fill:#e3f2fd
    style G fill:#c8e6c9
```

**R√®gles de mise √† jour** :
- w := w - Œ± √ó ‚àáw J
- b := b - Œ± √ó ‚àáb J

o√π Œ± est le taux d'apprentissage.

### Impl√©mentation Python

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import make_classification

# G√©n√©ration de donn√©es
X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, 
                          n_informative=2, n_clusters_per_class=1, random_state=42)

# Entra√Ænement
model = LogisticRegression()
model.fit(X, y)

# Pr√©dictions
y_pred = model.predict(X)
y_prob = model.predict_proba(X)

# √âvaluation
accuracy = accuracy_score(y, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print(f"Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_}")
```

---

## ‚öôÔ∏è Descente de gradient d√©taill√©e

### Algorithme g√©n√©ral

```mermaid
graph TD
    A[1. Initialisation al√©atoire] --> B[2. Calcul du co√ªt J]
    B --> C[3. Calcul des gradients ‚àáJ]
    C --> D[4. Mise √† jour des param√®tres]
    D --> E{5. Crit√®re d'arr√™t?}
    E -->|Non| B
    E -->|Oui| F[6. Param√®tres optimaux]
    
    subgraph "Crit√®res d'arr√™t"
        G[Nb max d'it√©rations]
        H[Variation de co√ªt < seuil]
        I[Norme du gradient < seuil]
    end
```

### Variants de la descente de gradient

#### 1. Batch Gradient Descent
- **Utilise** : Tout le dataset √† chaque it√©ration
- **Avantages** : Convergence stable
- **Inconv√©nients** : Lent sur gros datasets

#### 2. Stochastic Gradient Descent (SGD)
- **Utilise** : Un √©chantillon √† la fois
- **Avantages** : Rapide, peut √©chapper aux minima locaux
- **Inconv√©nients** : Convergence bruit√©e

#### 3. Mini-batch Gradient Descent
- **Utilise** : Petits lots d'√©chantillons
- **Avantages** : Compromis vitesse/stabilit√©
- **Inconv√©nients** : Hyperparam√®tre suppl√©mentaire (taille de batch)

### Hyperparam√®tres critiques

#### Taux d'apprentissage (Œ±)

```mermaid
graph LR
    subgraph "Taux trop petit"
        A[Convergence lente] --> B[Nombreuses it√©rations]
    end
    
    subgraph "Taux optimal"
        C[Convergence rapide] --> D[Minimum global]
    end
    
    subgraph "Taux trop grand"
        E[Oscillations] --> F[Divergence possible]
    end
```

**Strat√©gies d'adaptation** :
- **Learning rate scheduling** : Diminuer Œ± au fil du temps
- **Adaptive methods** : Adam, RMSprop (ajustement automatique)

---

## üìä √âvaluation et diagnostic

### M√©triques pour r√©gression lin√©aire

#### 1. R¬≤ (Coefficient de d√©termination)
**Interpr√©tation** : Proportion de variance expliqu√©e par le mod√®le

R¬≤ = 1 - (SS_res / SS_tot)

- **R¬≤ = 1** : Mod√®le parfait
- **R¬≤ = 0** : Mod√®le √©quivalent √† la moyenne
- **R¬≤ < 0** : Mod√®le pire que la moyenne

#### 2. Mean Absolute Error (MAE)
MAE = (1/n) √ó Œ£|y·µ¢ - ≈∑·µ¢|

**Avantage** : Robuste aux outliers

#### 3. Root Mean Square Error (RMSE)
RMSE = ‚àöMSE

**Avantage** : M√™me unit√© que la variable cible

### Diagnostic des probl√®mes

```mermaid
graph TD
    subgraph "Analyse des r√©sidus"
        A[R√©sidus vs Pr√©dictions] --> B{Pattern visible?}
        B -->|Oui| C[Non-lin√©arit√©]
        B -->|Non| D[Mod√®le appropri√©]
    end
    
    subgraph "Tests statistiques"
        E[Normalit√© des r√©sidus] --> F[Q-Q plot]
        G[Homosc√©dasticit√©] --> H[Test de Breusch-Pagan]
        I[Autocorr√©lation] --> J[Test de Durbin-Watson]
    end
```

### R√©gularisation

#### Probl√®me : Overfitting

**Sympt√¥mes** :
- Performance excellente sur donn√©es d'entra√Ænement
- Performance m√©diocre sur donn√©es de test
- Coefficients tr√®s √©lev√©s

#### Solutions : Ridge et Lasso

##### Ridge Regression (L2)
**P√©nalit√©** : Œª √ó Œ£w·µ¢¬≤

- R√©duit les coefficients sans les annuler
- Garde toutes les features

##### Lasso Regression (L1)
**P√©nalit√©** : Œª √ó Œ£|w·µ¢|

- Peut annuler certains coefficients
- S√©lection automatique de features

```python
from sklearn.linear_model import Ridge, Lasso

# Ridge
ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)

# Lasso
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
```

---

## üöÄ Applications pratiques

### R√©gression lin√©aire : Pr√©diction de prix

```python
# Exemple : Prix immobilier
features = ['surface', 'nombre_pieces', 'age', 'distance_centre']
target = 'prix'

# Preprocessing
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Mod√®le avec r√©gularisation
from sklearn.linear_model import ElasticNet
model = ElasticNet(alpha=0.1, l1_ratio=0.5)
model.fit(X_scaled, y)

# Interpr√©tation des coefficients
feature_importance = dict(zip(features, model.coef_))
print("Impact de chaque feature:")
for feature, coef in feature_importance.items():
    print(f"{feature}: {coef:.2f}")
```

### R√©gression logistique : Classification m√©dicale

```python
# Exemple : Diagnostic m√©dical
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, roc_curve

# Split des donn√©es
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Mod√®le
logistic = LogisticRegression(C=1.0)  # C = 1/Œª
logistic.fit(X_train, y_train)

# √âvaluation
y_prob = logistic.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_prob)
print(f"AUC-ROC: {auc:.3f}")

# Courbe ROC
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend()
plt.show()
```

---

## üéØ R√©capitulatif

**Points cl√©s √† retenir :**

### R√©gression lin√©aire
- **Mod√®le simple** mais puissant pour relations lin√©aires
- **Solution analytique** existe (moindres carr√©s)
- **Interpr√©table** : coefficients = impact des features

### R√©gression logistique
- **Classification** via fonction sigmo√Øde
- **Optimisation** par descente de gradient
- **Probabilit√©s** en sortie, pas juste des classes

### Bonnes pratiques
1. **Normaliser les features** pour la stabilit√© num√©rique
2. **R√©gularisation** pour √©viter l'overfitting
3. **Validation crois√©e** pour l'√©valuation
4. **Analyse des r√©sidus** pour le diagnostic

---

## üîó Pour aller plus loin

- **Extensions** : R√©gression polynomiale, interactions entre features
- **Algorithmes avanc√©s** : Elastic Net, r√©gression robuste
- **Optimisation** : M√©thodes adaptatives (Adam, AdaGrad)
- **Applications** : GLM (Generalized Linear Models), survival analysis