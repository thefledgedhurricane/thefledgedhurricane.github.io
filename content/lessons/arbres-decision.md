---
title: "Arbres de d√©cision et for√™ts al√©atoires"
description: "Ma√Ætrisez les algorithmes bas√©s sur les arbres pour classification et r√©gression"
difficulty: "intermediate" 
estimatedTime: "50 minutes"
keywords: ["arbres de d√©cision", "random forest", "bagging", "feature importance"]
---

# Arbres de d√©cision et for√™ts al√©atoires

## üéØ Objectifs d'apprentissage

√Ä la fin de cette le√ßon, vous serez capable de :
- ‚úÖ Comprendre le fonctionnement des arbres de d√©cision
- ‚úÖ Construire et interpr√©ter un arbre de d√©cision
- ‚úÖ Ma√Ætriser les for√™ts al√©atoires et le bagging
- ‚úÖ √âvaluer l'importance des features

---

## üå≥ Arbres de d√©cision

### Principe fondamental

Un **arbre de d√©cision** est un mod√®le qui pr√©dit une valeur cible en apprenant des r√®gles de d√©cision simples inf√©r√©es des features.

```mermaid
graph TD
    A[Outlook] --> B{Sunny}
    A --> C{Overcast}
    A --> D{Rain}
    
    B --> E[Humidity] 
    E --> F{‚â§70%}
    E --> G{>70%}
    F --> H[Play: Yes]
    G --> I[Play: No]
    
    C --> J[Play: Yes]
    
    D --> K[Wind]
    K --> L{Strong}
    K --> M{Weak}
    L --> N[Play: No]
    M --> O[Play: Yes]
    
    style H fill:#c8e6c9
    style I fill:#ffcdd2
    style J fill:#c8e6c9
    style N fill:#ffcdd2
    style O fill:#c8e6c9
```

### Structure d'un arbre

#### Composants essentiels

```mermaid
graph TD
    subgraph "Anatomie d'un arbre"
        A[N≈ìud racine<br/>Premi√®re division] --> B[N≈ìud interne<br/>Condition de test]
        A --> C[N≈ìud interne<br/>Condition de test]
        
        B --> D[Feuille<br/>Pr√©diction finale]
        B --> E[Feuille<br/>Pr√©diction finale]
        
        C --> F[N≈ìud interne<br/>Subdivision]
        C --> G[Feuille<br/>Pr√©diction finale]
        
        F --> H[Feuille<br/>Pr√©diction finale]
        F --> I[Feuille<br/>Pr√©diction finale]
    end
    
    style A fill:#e3f2fd
    style D fill:#c8e6c9
    style E fill:#c8e6c9
    style G fill:#c8e6c9
    style H fill:#c8e6c9
    style I fill:#c8e6c9
```

- **N≈ìud racine** : Point de d√©part, divise tout l'ensemble
- **N≈ìuds internes** : Tests sur les features
- **Branches** : R√©sultats possibles du test
- **Feuilles** : Pr√©dictions finales

### Algorithme de construction

#### Approche r√©cursive

```mermaid
flowchart TD
    A[Ensemble de donn√©es] --> B{Pure ou<br/>crit√®re d'arr√™t?}
    B -->|Oui| C[Cr√©er feuille<br/>Classe majoritaire]
    B -->|Non| D[Trouver meilleure division]
    D --> E[Diviser l'ensemble]
    E --> F[Sous-ensemble gauche]
    E --> G[Sous-ensemble droit]
    F --> A1[R√©cursion gauche]
    G --> A2[R√©cursion droite]
    
    style C fill:#c8e6c9
    style D fill:#fff3e0
```

#### Crit√®res de division

**1. Entropie (Information Gain)**

Entropie(S) = -Œ£ p·µ¢ log‚ÇÇ(p·µ¢)

o√π p·µ¢ est la proportion de la classe i.

**Gain d'information** = Entropie(parent) - Œ£ (|S·µ•|/|S|) √ó Entropie(S·µ•)

**2. Indice de Gini**

Gini(S) = 1 - Œ£ p·µ¢¬≤

**Gini Impurity** mesure la probabilit√© qu'un √©l√©ment soit mal class√©.

**3. Variance (pour la r√©gression)**

Variance(S) = Œ£ (y·µ¢ - »≥)¬≤ / n

```mermaid
graph LR
    subgraph "Comparaison des crit√®res"
        A[Entropie] --> D[Plus sensible aux d√©s√©quilibres]
        B[Gini] --> E[Calcul plus rapide]
        C[Variance] --> F[Pour r√©gression uniquement]
    end
```

### Exemple de construction pas √† pas

#### Dataset exemple : Jouer au tennis

| Outlook | Temperature | Humidity | Wind | Play |
|---------|-------------|----------|------|------|
| Sunny | Hot | High | Weak | No |
| Sunny | Hot | High | Strong | No |
| Overcast | Hot | High | Weak | Yes |
| Rain | Mild | High | Weak | Yes |
| Rain | Cool | Normal | Weak | Yes |
| Rain | Cool | Normal | Strong | No |
| Overcast | Cool | Normal | Strong | Yes |

#### Calcul du gain d'information

**Entropie initiale** :
- Play=Yes: 4/7, Play=No: 3/7
- Entropie = -(4/7)log‚ÇÇ(4/7) - (3/7)log‚ÇÇ(3/7) ‚âà 0.985

**Division par Outlook** :
- Sunny: 2 Yes, 2 No ‚Üí Entropie = 1.0
- Overcast: 2 Yes, 0 No ‚Üí Entropie = 0.0  
- Rain: 2 Yes, 1 No ‚Üí Entropie ‚âà 0.918

**Gain** = 0.985 - (3/7)√ó1.0 - (2/7)√ó0.0 - (2/7)√ó0.918 ‚âà 0.246

### Impl√©mentation Python

```python
from sklearn.tree import DecisionTreeClassifier, export_text, plot_tree
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Chargement des donn√©es
iris = load_iris()
X, y = iris.data, iris.target

# Cr√©ation du mod√®le
dt = DecisionTreeClassifier(
    criterion='gini',           # ou 'entropy'
    max_depth=3,               # Profondeur maximale
    min_samples_split=2,       # Min √©chantillons pour diviser
    min_samples_leaf=1,        # Min √©chantillons par feuille
    random_state=42
)

# Entra√Ænement
dt.fit(X, y)

# Visualisation textuelle
tree_rules = export_text(dt, feature_names=iris.feature_names)
print(tree_rules)

# Visualisation graphique
plt.figure(figsize=(12, 8))
plot_tree(dt, feature_names=iris.feature_names, class_names=iris.target_names, 
          filled=True, rounded=True)
plt.show()

# Importance des features
importances = dt.feature_importances_
for i, importance in enumerate(importances):
    print(f"{iris.feature_names[i]}: {importance:.3f}")
```

---

## üå≤ Probl√®mes des arbres simples

### 1. Overfitting

```mermaid
graph TD
    subgraph "Arbre profond (overfitting)"
        A[Racine] --> B[...]
        B --> C[...]
        C --> D[Feuille tr√®s sp√©cifique]
        C --> E[Feuille tr√®s sp√©cifique]
    end
    
    subgraph "Cons√©quences"
        F[M√©morise le bruit] --> G[Mauvaise g√©n√©ralisation]
        H[Performance train √©lev√©e] --> I[Performance test faible]
    end
```

### 2. Instabilit√©

**Petit changement dans les donn√©es** ‚Üí **Arbre compl√®tement diff√©rent**

### 3. Biais

- **Biais vers features num√©riques** (plus de points de coupure possibles)
- **Biais vers features avec plus de modalit√©s**

---

## üåä Solutions : Ensemble Methods

### Bagging (Bootstrap Aggregating)

#### Principe

```mermaid
flowchart TD
    A[Dataset original<br/>N √©chantillons] --> B[Bootstrap 1<br/>N √©chantillons avec remise]
    A --> C[Bootstrap 2<br/>N √©chantillons avec remise] 
    A --> D[Bootstrap k<br/>N √©chantillons avec remise]
    
    B --> E[Arbre 1]
    C --> F[Arbre 2]
    D --> G[Arbre k]
    
    E --> H[Pr√©diction 1]
    F --> I[Pr√©diction 2]
    G --> J[Pr√©diction k]
    
    H --> K[Vote majoritaire<br/>ou moyenne]
    I --> K
    J --> K
    
    K --> L[Pr√©diction finale]
    
    style L fill:#c8e6c9
```

#### Avantages du bagging

1. **R√©duction de variance** : Moyenne de pr√©dictions plus stable
2. **Parall√©lisation** : Arbres ind√©pendants, entra√Ænement parall√®le
3. **Robustesse** : Moins sensible au bruit et aux outliers

---

## üå≥ Random Forest

### Principe : Bagging + Randomisation des features

#### Innovation cl√©

√Ä chaque n≈ìud, consid√©rer seulement un **sous-ensemble al√©atoire** des features.

```mermaid
graph TD
    subgraph "Arbre classique"
        A[Toutes les features] --> B[Meilleure division]
    end
    
    subgraph "Random Forest"
        C[Sous-ensemble al√©atoire<br/>‚àöp features] --> D[Meilleure division<br/>parmi le sous-ensemble]
    end
    
    style D fill:#c8e6c9
```

#### Algorithme complet

```mermaid
flowchart TD
    A[Pour chaque arbre t = 1 √† T] --> B[Bootstrap √©chantillon S‚Çú]
    B --> C[Pour chaque n≈ìud]
    C --> D[S√©lectionner ‚àöp features al√©atoires]
    D --> E[Trouver meilleure division]
    E --> F[Diviser le n≈ìud]
    F --> G{Crit√®re d'arr√™t?}
    G -->|Non| C
    G -->|Oui| H[Arbre t termin√©]
    H --> I{t < T?}
    I -->|Oui| A
    I -->|Non| J[For√™t compl√®te]
    
    style J fill:#c8e6c9
```

### Hyperparam√®tres importants

#### 1. Nombre d'arbres (n_estimators)
- **Plus d'arbres** = moins de variance mais plus de temps
- **R√®gle pratique** : Commencer par 100

#### 2. Nombre de features par n≈ìud (max_features)
- **Classification** : ‚àöp (par d√©faut)
- **R√©gression** : p/3 (par d√©faut)
- **Auto-tuning** recommand√©

#### 3. Profondeur maximale (max_depth)
- **Arbres profonds** pour le bagging (contrairement aux arbres simples)
- **None** (pas de limite) souvent optimal

#### 4. √âchantillons minimum
- **min_samples_split** : 2 (d√©faut)
- **min_samples_leaf** : 1 (d√©faut)
- Augmenter pour r√©duire l'overfitting

### Impl√©mentation Python

```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Classification
rf_clf = RandomForestClassifier(
    n_estimators=100,          # Nombre d'arbres
    max_features='sqrt',       # ‚àöp features par n≈ìud
    max_depth=None,           # Pas de limite de profondeur
    min_samples_split=2,      # Min pour diviser
    min_samples_leaf=1,       # Min par feuille
    bootstrap=True,           # √âchantillonnage avec remise
    oob_score=True,          # Score out-of-bag
    random_state=42,
    n_jobs=-1                # Parall√©lisation
)

# Entra√Ænement
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
rf_clf.fit(X_train, y_train)

# Pr√©dictions
y_pred = rf_clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.3f}")

# Score OOB (estimation sans validation set)
print(f"OOB Score: {rf_clf.oob_score_:.3f}")

# Importance des features
feature_importance = rf_clf.feature_importances_
feature_names = ['feature_' + str(i) for i in range(X.shape[1])]

import pandas as pd
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print(importance_df)
```

---

## üìä Importance des features

### M√©thodes de calcul

#### 1. Importance bas√©e sur l'impuret√© (MDI)

**Principe** : Mesurer la r√©duction d'impuret√© apport√©e par chaque feature.

Importance(feature) = Œ£ (r√©duction d'impuret√© lors des divisions utilisant cette feature)

#### 2. Importance par permutation

**Principe** : Mesurer la d√©gradation de performance quand on permute al√©atoirement une feature.

```mermaid
flowchart TD
    A[Performance de r√©f√©rence] --> B[Permuter feature X]
    B --> C[Nouvelle performance]
    C --> D[Importance = Performance_ref - Performance_perm]
    
    subgraph "Avantages"
        E[Ind√©pendant du mod√®le]
        F[Capture les interactions]
        G[Plus fiable]
    end
```

#### Impl√©mentation

```python
from sklearn.inspection import permutation_importance
import matplotlib.pyplot as plt

# Importance par impuret√© (built-in)
mdi_importance = rf_clf.feature_importances_

# Importance par permutation
perm_importance = permutation_importance(rf_clf, X_test, y_test, 
                                       n_repeats=10, random_state=42)

# Visualisation comparative
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

# MDI
ax1.barh(range(len(mdi_importance)), mdi_importance)
ax1.set_title('MDI Importance')
ax1.set_xlabel('Importance')

# Permutation
ax2.barh(range(len(perm_importance.importances_mean)), 
         perm_importance.importances_mean)
ax2.set_title('Permutation Importance')
ax2.set_xlabel('Importance')

plt.tight_layout()
plt.show()
```

---

## üöÄ Applications avanc√©es

### 1. Gestion des donn√©es manquantes

#### Surrogate splits

```mermaid
graph TD
    A[Division principale:<br/>Feature A < 5] --> B[Donn√©es compl√®tes]
    A --> C[Valeurs manquantes<br/>pour Feature A]
    
    C --> D[Division surrogate:<br/>Feature B < 10]
    D --> E[Approximation de<br/>la division principale]
```

#### Random Forest avec valeurs manquantes

```python
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# Pipeline avec imputation
pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))
])

pipeline.fit(X_train, y_train)
y_pred = pipeline.predict(X_test)
```

### 2. D√©tection d'outliers

#### Isolation Forest

**Principe** : Outliers sont plus faciles √† isoler (moins de divisions n√©cessaires)

```python
from sklearn.ensemble import IsolationForest

# D√©tection d'anomalies
iso_forest = IsolationForest(contamination=0.1, random_state=42)
outliers = iso_forest.fit_predict(X)

# -1 = outlier, 1 = normal
print(f"Nombre d'outliers d√©tect√©s: {sum(outliers == -1)}")
```

### 3. Feature selection

#### √âlimination r√©cursive avec Random Forest

```python
from sklearn.feature_selection import RFE

# S√©lection de 5 meilleures features
rf = RandomForestClassifier(n_estimators=50, random_state=42)
rfe = RFE(estimator=rf, n_features_to_select=5, step=1)
rfe.fit(X_train, y_train)

# Features s√©lectionn√©es
selected_features = [feature_names[i] for i in range(len(feature_names)) if rfe.support_[i]]
print("Features s√©lectionn√©es:", selected_features)
```

---

## ‚ö° Optimisation et tuning

### Grid Search pour hyperparam√®tres

```python
from sklearn.model_selection import GridSearchCV

# Grille de param√®tres
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20],
    'max_features': ['sqrt', 'log2', None],
    'min_samples_split': [2, 5, 10]
}

# Grid search avec validation crois√©e
grid_search = GridSearchCV(
    RandomForestClassifier(random_state=42),
    param_grid,
    cv=5,
    scoring='accuracy',
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

print("Meilleurs param√®tres:", grid_search.best_params_)
print("Meilleur score:", grid_search.best_score_)

# Mod√®le optimal
best_rf = grid_search.best_estimator_
```

### Analyse de l'apprentissage

```python
from sklearn.model_selection import learning_curve

# Courbes d'apprentissage
train_sizes, train_scores, val_scores = learning_curve(
    RandomForestClassifier(n_estimators=100, random_state=42),
    X, y, cv=5, n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10)
)

# Visualisation
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training score')
plt.plot(train_sizes, np.mean(val_scores, axis=1), 'o-', label='Validation score')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy Score')
plt.legend()
plt.title('Learning Curves - Random Forest')
plt.show()
```

---

## üéØ R√©capitulatif

**Points cl√©s √† retenir :**

### Arbres de d√©cision
- **Interpr√©tables** et **intuitifs**
- **Tendance √† l'overfitting** si pas contraints
- **Instables** (sensibles aux donn√©es)

### Random Forest
- **Combine** bagging + randomisation des features
- **R√©duction significative** de la variance
- **Performance robuste** sur de nombreux probl√®mes
- **Parall√©lisable** et **scalable**

### Bonnes pratiques
1. **Commencer simple** : arbres de d√©cision pour comprendre
2. **Random Forest** pour la performance
3. **Tuning des hyperparam√®tres** crucial
4. **Importance des features** pour l'interpr√©tation
5. **Validation crois√©e** pour l'√©valuation fiable

### Quand utiliser ?
- **Classification/r√©gression** traditionnelle
- **Features h√©t√©rog√®nes** (num√©riques + cat√©gorielles)
- **Besoin d'interpr√©tabilit√©** (arbres simples)
- **Baseline robuste** (Random Forest)

---

## üîó Pour aller plus loin

- **Gradient Boosting** : XGBoost, LightGBM, CatBoost
- **Extra Trees** : Randomisation suppl√©mentaire
- **Isolation Forest** : D√©tection d'anomalies
- **Multi-output** : Pr√©diction simultan√©e de plusieurs cibles