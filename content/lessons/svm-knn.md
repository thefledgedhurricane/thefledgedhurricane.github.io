---
title: "SVM et k-NN : m√©thodes g√©om√©triques"
description: "Explorez les Support Vector Machines et k-Nearest Neighbors, deux approches g√©om√©triques puissantes"
difficulty: "intermediate"
estimatedTime: "45 minutes"
keywords: ["SVM", "support vector machines", "k-NN", "kernel trick", "g√©om√©trie"]
---

# SVM et k-NN : m√©thodes g√©om√©triques

## üéØ Objectifs d'apprentissage

√Ä la fin de cette le√ßon, vous serez capable de :
- ‚úÖ Comprendre les principes g√©om√©triques du SVM
- ‚úÖ Ma√Ætriser le kernel trick et les noyaux
- ‚úÖ Impl√©menter et optimiser k-NN
- ‚úÖ Choisir entre SVM et k-NN selon le contexte

---

## üéØ Support Vector Machines (SVM)

### Intuition g√©om√©trique

Le **SVM** cherche √† trouver l'**hyperplan optimal** qui s√©pare les classes avec la **marge maximale**.

```mermaid
graph TD
    subgraph "Principe SVM"
        A[Points de donn√©es] --> B[Hyperplan s√©parateur]
        B --> C[Maximisation de la marge]
        C --> D[Support vectors]
    end
    
    subgraph "Avantages"
        E[Marge maximale] --> F[Bonne g√©n√©ralisation]
        G[Support vectors uniquement] --> H[Mod√®le compact]
        I[Kernel trick] --> J[Non-lin√©arit√©]
    end
```

### Cas lin√©airement s√©parable

#### D√©finition de l'hyperplan

**√âquation** : w^T x + b = 0

o√π :
- **w** : vecteur normal √† l'hyperplan
- **b** : biais (offset)
- **x** : point dans l'espace

#### Marge g√©om√©trique

La **marge** est la distance minimale entre l'hyperplan et les points les plus proches de chaque classe.

```mermaid
graph LR
    subgraph "G√©om√©trie SVM"
        A[Classe +1] --> B[Support vector +]
        B --> C[Hyperplan H+: wx + b = +1]
        C --> D[Hyperplan optimal: wx + b = 0]
        D --> E[Hyperplan H-: wx + b = -1]
        E --> F[Support vector -]
        F --> G[Classe -1]
    end
```

**Marge** = 2/||w||

**Objectif** : Maximiser la marge ‚ü∫ Minimiser ||w||¬≤

#### Formulation d'optimisation

**Probl√®me primal** :
- Minimiser : (1/2)||w||¬≤
- Sous contraintes : y·µ¢(w^T x·µ¢ + b) ‚â• 1, ‚àÄi

```mermaid
flowchart TD
    A[Probl√®me d'optimisation<br/>quadratique convexe] --> B[Lagrangien]
    B --> C[Conditions KKT]
    C --> D[Probl√®me dual]
    D --> E[Support vectors]
    E --> F[Solution optimale]
    
    style F fill:#c8e6c9
```

### Cas non-lin√©airement s√©parable

#### Slack variables (variables de rel√¢chement)

Permettre quelques erreurs de classification avec p√©nalit√©.

**Probl√®me modifi√©** :
- Minimiser : (1/2)||w||¬≤ + C Œ£Œæ·µ¢
- Sous contraintes : y·µ¢(w^T x·µ¢ + b) ‚â• 1 - Œæ·µ¢, Œæ·µ¢ ‚â• 0

o√π :
- **Œæ·µ¢** : variables de rel√¢chement
- **C** : param√®tre de r√©gularisation

```mermaid
graph TD
    subgraph "Effet du param√®tre C"
        A[C petit] --> B[Marge large<br/>Plus d'erreurs]
        C[C grand] --> D[Marge √©troite<br/>Moins d'erreurs]
    end
    
    subgraph "Trade-off"
        E[Complexit√© du mod√®le] --> F[vs]
        F --> G[Erreur d'entra√Ænement]
    end
```

---

## üîÑ Kernel Trick

### Motivation

**Probl√®me** : Donn√©es non-lin√©airement s√©parables dans l'espace original

**Solution** : Projeter dans un espace de dimension sup√©rieure o√π elles deviennent lin√©airement s√©parables

```mermaid
graph LR
    subgraph "Espace original (2D)"
        A[Classes m√©lang√©es] --> B[Non s√©parable<br/>lin√©airement]
    end
    
    subgraph "Espace transform√© (3D+)"
        C[œÜ(x): transformation] --> D[Classes s√©parables<br/>lin√©airement]
    end
    
    A --> C
```

### Astuce math√©matique

Au lieu de calculer œÜ(x) explicitement, utiliser directement K(x·µ¢, x‚±º) = œÜ(x·µ¢)^T œÜ(x‚±º)

**Avantage** : √âvite le calcul co√ªteux dans l'espace de haute dimension

### Noyaux populaires

#### 1. Noyau lin√©aire
K(x·µ¢, x‚±º) = x·µ¢^T x‚±º

**Usage** : Donn√©es d√©j√† lin√©airement s√©parables

#### 2. Noyau polynomial
K(x·µ¢, x‚±º) = (Œ≥ x·µ¢^T x‚±º + r)^d

**Param√®tres** :
- **d** : degr√© du polyn√¥me
- **Œ≥** : coefficient d'√©chelle
- **r** : terme constant

#### 3. Noyau RBF (Radial Basis Function)
K(x·µ¢, x‚±º) = exp(-Œ≥ ||x·µ¢ - x‚±º||¬≤)

**Caract√©ristiques** :
- Plus populaire et polyvalent
- **Œ≥** contr√¥le la "largeur" de l'influence
- Dimension infinie implicite

#### 4. Noyau sigmo√Øde
K(x·µ¢, x‚±º) = tanh(Œ≥ x·µ¢^T x‚±º + r)

**Usage** : Moins fr√©quent, similaire aux r√©seaux de neurones

```mermaid
graph TD
    subgraph "Choix du noyau"
        A[Donn√©es lin√©aires] --> B[Lin√©aire]
        C[Donn√©es polynomiales] --> D[Polynomial]
        E[Donn√©es complexes] --> F[RBF]
        G[Cas g√©n√©ral] --> F
    end
```

### Impl√©mentation Python

```python
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

# G√©n√©ration de donn√©es non-lin√©aires
from sklearn.datasets import make_circles
X, y = make_circles(n_samples=1000, noise=0.1, factor=0.3, random_state=42)

# Normalisation (importante pour SVM)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# SVM avec diff√©rents noyaux
kernels = ['linear', 'poly', 'rbf', 'sigmoid']
results = {}

for kernel in kernels:
    svm = SVC(kernel=kernel, random_state=42)
    svm.fit(X_train, y_train)
    score = svm.score(X_test, y_test)
    results[kernel] = score
    print(f"{kernel.capitalize()} kernel accuracy: {score:.3f}")

# Meilleur noyau
best_kernel = max(results, key=results.get)
print(f"\nMeilleur noyau: {best_kernel}")
```

### Optimisation des hyperparam√®tres

```python
# Grid search pour SVM RBF
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.01, 0.1, 1, 10]
}

svm_rbf = SVC(kernel='rbf', random_state=42)
grid_search = GridSearchCV(svm_rbf, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Meilleurs param√®tres:", grid_search.best_params_)
print("Meilleur score CV:", grid_search.best_score_)

# Mod√®le final
best_svm = grid_search.best_estimator_
test_score = best_svm.score(X_test, y_test)
print(f"Score sur test: {test_score:.3f}")
```

---

## üéØ k-Nearest Neighbors (k-NN)

### Principe fondamental

**k-NN** classe un point en se basant sur les **k voisins les plus proches** dans l'espace des features.

```mermaid
graph TD
    A[Point √† classer] --> B[Calculer distances<br/>√† tous les points]
    B --> C[S√©lectionner k<br/>plus proches voisins]
    C --> D{Classification}
    D --> E[Vote majoritaire]
    C --> F{R√©gression}
    F --> G[Moyenne des valeurs]
    
    style E fill:#e3f2fd
    style G fill:#fff3e0
```

### Algorithme d√©taill√©

#### Classification

```python
def knn_classify(X_train, y_train, x_query, k):
    # 1. Calculer distances
    distances = [euclidean_distance(x_query, x_train) for x_train in X_train]
    
    # 2. Trouver k plus proches voisins
    k_indices = sorted(range(len(distances)), key=lambda i: distances[i])[:k]
    
    # 3. Vote majoritaire
    k_labels = [y_train[i] for i in k_indices]
    return most_frequent(k_labels)
```

#### M√©triques de distance

**1. Distance euclidienne** (la plus courante)
d(x, y) = ‚àöŒ£(x·µ¢ - y·µ¢)¬≤

**2. Distance de Manhattan**
d(x, y) = Œ£|x·µ¢ - y·µ¢|

**3. Distance de Minkowski**
d(x, y) = (Œ£|x·µ¢ - y·µ¢|^p)^(1/p)

**4. Distance de Hamming** (variables cat√©gorielles)
d(x, y) = Œ£(x·µ¢ ‚â† y·µ¢)

```mermaid
graph LR
    subgraph "Choix de distance"
        A[Features continues] --> B[Euclidienne]
        C[Features ordinales] --> D[Manhattan]
        E[Features cat√©gorielles] --> F[Hamming]
        G[Donn√©es haute dimension] --> H[Cosinus]
    end
```

### Impl√©mentation et optimisation

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import time

# Dataset exemple
from sklearn.datasets import load_digits
digits = load_digits()
X, y = digits.data, digits.target

# Normalisation (importante pour k-NN)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)

# Test de diff√©rentes valeurs de k
k_range = range(1, 31)
accuracies = []
times = []

for k in k_range:
    knn = KNeighborsClassifier(n_neighbors=k)
    
    start_time = time.time()
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    end_time = time.time()
    
    accuracy = accuracy_score(y_test, y_pred)
    accuracies.append(accuracy)
    times.append(end_time - start_time)

# Visualisation
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(k_range, accuracies, 'b-', marker='o')
plt.xlabel('k')
plt.ylabel('Accuracy')
plt.title('Accuracy vs k')

plt.subplot(1, 2, 2)
plt.plot(k_range, times, 'r-', marker='s')
plt.xlabel('k')
plt.ylabel('Time (seconds)')
plt.title('Training time vs k')

plt.tight_layout()
plt.show()

# k optimal
optimal_k = k_range[np.argmax(accuracies)]
print(f"k optimal: {optimal_k}")
print(f"Meilleure accuracy: {max(accuracies):.3f}")
```

### Optimisation de performance

#### 1. Structures de donn√©es efficaces

**KD-Tree** : Efficace en faible dimension (< 20)
```python
knn_kdtree = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')
```

**Ball Tree** : Efficace en haute dimension
```python
knn_balltree = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')
```

**Brute force** : Calcul direct des distances
```python
knn_brute = KNeighborsClassifier(n_neighbors=5, algorithm='brute')
```

#### 2. Approximation pour gros datasets

**LSH (Locality Sensitive Hashing)** : Approximation rapide

```python
from sklearn.neighbors import NearestNeighbors

# Recherche approximative
nn = NearestNeighbors(n_neighbors=5, metric='cosine')
nn.fit(X_train)

# Recherche des voisins
distances, indices = nn.kneighbors(X_test[:10])
```

---

## ‚öñÔ∏è Comparaison SVM vs k-NN

### Tableau comparatif

| Aspect | SVM | k-NN |
|--------|-----|------|
| **Complexit√© d'entra√Ænement** | O(n¬≥) √† O(n¬≤) | O(1) (lazy learning) |
| **Complexit√© de pr√©diction** | O(n_sv) | O(n) ou O(log n) avec index |
| **M√©moire** | Support vectors seulement | Tout le dataset |
| **Performance** | Excellente avec bon noyau | Bonne si k bien choisi |
| **Interpr√©tabilit√©** | Faible (sauf lin√©aire) | √âlev√©e |
| **Robustesse au bruit** | Bonne (marge) | Faible |
| **Donn√©es haute dimension** | Excellente | Probl√©matique (curse) |
| **Donn√©es d√©s√©quilibr√©es** | Probl√©matique | Probl√©matique |

### Crit√®res de choix

```mermaid
flowchart TD
    A[Choix algorithme] --> B{Taille dataset}
    B -->|Petit < 10k| C[k-NN possible]
    B -->|Grand > 100k| D[SVM pr√©f√©rable]
    
    C --> E{Dimension}
    E -->|Faible < 20| F[k-NN optimal]
    E -->|√âlev√©e > 50| G[SVM avec RBF]
    
    D --> H{Lin√©arit√©}
    H -->|Lin√©aire| I[SVM lin√©aire]
    H -->|Non-lin√©aire| J[SVM RBF]
    
    style F fill:#c8e6c9
    style G fill:#c8e6c9
    style I fill:#c8e6c9
    style J fill:#c8e6c9
```

---

## üöÄ Applications pratiques

### SVM pour classification de texte

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline

# Pipeline SVM + TF-IDF
text_classifier = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000, stop_words='english')),
    ('svm', SVC(kernel='linear', C=1.0))
])

# Donn√©es d'exemple (sentiment analysis)
texts = ["I love this movie", "This is terrible", "Great film!", "Awful acting"]
labels = [1, 0, 1, 0]  # 1=positive, 0=negative

text_classifier.fit(texts, labels)

# Pr√©diction
new_texts = ["Amazing movie!", "Bad story"]
predictions = text_classifier.predict(new_texts)
print(f"Pr√©dictions: {predictions}")
```

### k-NN pour syst√®me de recommandation

```python
from sklearn.neighbors import NearestNeighbors
import pandas as pd

# Matrice utilisateur-item (exemple)
user_item_matrix = pd.DataFrame({
    'user_1': [5, 3, 0, 1],
    'user_2': [4, 0, 0, 1], 
    'user_3': [1, 1, 0, 5],
    'user_4': [1, 0, 0, 4],
    'user_5': [0, 1, 5, 4]
}, index=['item_A', 'item_B', 'item_C', 'item_D'])

# k-NN pour trouver utilisateurs similaires
nn_model = NearestNeighbors(n_neighbors=3, metric='cosine')
nn_model.fit(user_item_matrix.T)  # Transpose: users en rows

# Recommandations pour user_1
distances, indices = nn_model.kneighbors([user_item_matrix['user_1']])
similar_users = user_item_matrix.columns[indices[0][1:]]  # Exclure user_1 lui-m√™me

print(f"Utilisateurs similaires √† user_1: {list(similar_users)}")
```

---

## üéØ Techniques avanc√©es

### SVM multiclasse

#### Strat√©gies de d√©composition

**1. One-vs-One (OvO)**
- n(n-1)/2 classificateurs binaires
- Chaque paire de classes

**2. One-vs-Rest (OvR)**
- n classificateurs binaires
- Chaque classe vs toutes les autres

```python
from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier

# OvO
ovo_svm = OneVsOneClassifier(SVC(kernel='rbf'))
ovo_svm.fit(X_train, y_train)

# OvR  
ovr_svm = OneVsRestClassifier(SVC(kernel='rbf'))
ovr_svm.fit(X_train, y_train)
```

### k-NN pond√©r√©

#### Pond√©ration par distance

Plus un voisin est proche, plus son vote compte.

```python
# k-NN avec pond√©ration par distance
knn_weighted = KNeighborsClassifier(
    n_neighbors=5,
    weights='distance'  # ou 'uniform'
)
knn_weighted.fit(X_train, y_train)
```

#### Pond√©ration adaptative

```python
def adaptive_knn(X_train, y_train, x_query, k_max=20):
    distances = [euclidean_distance(x_query, x) for x in X_train]
    sorted_indices = sorted(range(len(distances)), key=lambda i: distances[i])
    
    # Adaptation du k selon la densit√© locale
    local_density = estimate_density(x_query, X_train, radius=1.0)
    k = min(k_max, max(3, int(local_density * 10)))
    
    # Classification avec k adaptatif
    k_indices = sorted_indices[:k]
    k_labels = [y_train[i] for i in k_indices]
    return most_frequent(k_labels), k
```

---

## üéØ R√©capitulatif

**Points cl√©s √† retenir :**

### SVM
- **G√©om√©trie** : Marge maximale entre classes
- **Kernel trick** : Non-lin√©arit√© sans calcul explicite
- **Robuste** mais sensible aux param√®tres
- **Excellent** pour haute dimension et texte

### k-NN
- **Simplicit√©** : Pas d'entra√Ænement, conceptuellement simple
- **Versatilit√©** : Classification et r√©gression
- **Local** : Pr√©dictions bas√©es sur voisinage imm√©diat
- **Probl√©matique** en haute dimension

### Bonnes pratiques
1. **Normalisation obligatoire** pour les deux m√©thodes
2. **Cross-validation** pour choix des hyperparam√®tres
3. **SVM** pour probl√®mes complexes, **k-NN** pour prototypage rapide
4. **Consid√©rer la taille** du dataset pour le choix

### Applications typiques
- **SVM** : Classification de texte, bioinformatique, vision
- **k-NN** : Recommandations, d√©tection d'anomalies, baseline

---

## üîó Pour aller plus loin

- **SVM avanc√©** : Nu-SVM, Least Squares SVM
- **k-NN variants** : Radius neighbors, Local Outlier Factor
- **Approximation** : Random projections, hashing
- **Ensemble methods** : Bagging de k-NN, SVM committees