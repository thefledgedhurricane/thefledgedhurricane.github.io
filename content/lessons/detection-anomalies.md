---
title: "D√©tection d'anomalies : Isolation Forest et One-Class SVM"
description: "Ma√Ætrisez les techniques de d√©tection d'anomalies pour identifier les points de donn√©es atypiques"
difficulty: "intermediate"
estimatedTime: "35 minutes"
keywords: ["anomalies", "outliers", "isolation forest", "one-class SVM", "d√©tection", "surveillance"]
---

# D√©tection d'anomalies : Isolation Forest et One-Class SVM

## üéØ Objectifs d'apprentissage

√Ä la fin de cette le√ßon, vous serez capable de :
- ‚úÖ Comprendre les diff√©rents types d'anomalies
- ‚úÖ Impl√©menter Isolation Forest pour la d√©tection d'anomalies
- ‚úÖ Utiliser One-Class SVM pour des cas complexes
- ‚úÖ √âvaluer et comparer les m√©thodes de d√©tection

---

## üîç Qu'est-ce qu'une anomalie ?

### D√©finition

Une **anomalie** (ou **outlier**) est une observation qui **d√©vie significativement** du comportement normal ou attendu des donn√©es.

```mermaid
graph TD
    A[Donn√©es] --> B{Type d'anomalie}
    
    B --> C[Point aberrant<br/>Point outlier]
    B --> D[Anomalie contextuelle<br/>Contextual outlier]
    B --> E[Anomalie collective<br/>Collective outlier]
    
    C --> F["Exemple: Salaire de 1M‚Ç¨<br/>dans dataset √©tudiants"]
    D --> G["Exemple: 30¬∞C en d√©cembre<br/>√† Paris"]
    E --> H["Exemple: Pic de trafic<br/>non justifi√©"]
    
    style F fill:#ffcdd2
    style G fill:#fff3e0
    style H fill:#e1f5fe
```

### Types d'anomalies

#### 1. Anomalies globales (Point outliers)

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns

# G√©n√©ration de donn√©es avec anomalies globales
np.random.seed(42)
normal_data = np.random.normal(50, 10, 1000)
outliers = np.array([10, 15, 85, 90, 95])  # Points aberrants
data_with_outliers = np.concatenate([normal_data, outliers])

plt.figure(figsize=(12, 4))

plt.subplot(1, 3, 1)
plt.hist(normal_data, bins=30, alpha=0.7, color='blue', label='Donn√©es normales')
plt.xlabel('Valeur')
plt.ylabel('Fr√©quence')
plt.title('Distribution normale')
plt.legend()

plt.subplot(1, 3, 2)
plt.hist(data_with_outliers, bins=35, alpha=0.7, color='green', label='Avec anomalies')
plt.xlabel('Valeur')
plt.ylabel('Fr√©quence')
plt.title('Distribution avec anomalies globales')
plt.legend()

plt.subplot(1, 3, 3)
plt.boxplot(data_with_outliers)
plt.ylabel('Valeur')
plt.title('Box plot - Anomalies visibles')

plt.tight_layout()
plt.show()

print(f"Donn√©es normales: Œº={np.mean(normal_data):.1f}, œÉ={np.std(normal_data):.1f}")
print(f"Anomalies ajout√©es: {outliers}")
```

#### 2. Anomalies contextuelles

```python
# Simulation donn√©es temporelles avec anomalies contextuelles
dates = np.arange('2023-01', '2024-01', dtype='datetime64[D]')
n_days = len(dates)

# Tendance saisonni√®re normale
seasonal_pattern = 20 + 15 * np.sin(2 * np.pi * np.arange(n_days) / 365)
noise = np.random.normal(0, 2, n_days)
normal_series = seasonal_pattern + noise

# Injection d'anomalies contextuelles
anomalous_series = normal_series.copy()
# Pic de chaleur en hiver (contextuel)
winter_anomaly_idx = 50  # F√©vrier
anomalous_series[winter_anomaly_idx] = 35  # 35¬∞C en f√©vrier !

# Froid en √©t√© (contextuel)
summer_anomaly_idx = 200  # Juillet
anomalous_series[summer_anomaly_idx] = 5   # 5¬∞C en juillet !

plt.figure(figsize=(15, 8))

plt.subplot(2, 1, 1)
plt.plot(dates, normal_series, 'b-', alpha=0.7, label='S√©rie normale')
plt.ylabel('Temp√©rature (¬∞C)')
plt.title('S√©rie temporelle normale')
plt.legend()
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(dates, anomalous_series, 'g-', alpha=0.7, label='S√©rie avec anomalies')
plt.scatter(dates[winter_anomaly_idx], anomalous_series[winter_anomaly_idx], 
           color='red', s=100, zorder=5, label='Anomalie hiver')
plt.scatter(dates[summer_anomaly_idx], anomalous_series[summer_anomaly_idx], 
           color='red', s=100, zorder=5, label='Anomalie √©t√©')
plt.ylabel('Temp√©rature (¬∞C)')
plt.xlabel('Date')
plt.title('S√©rie avec anomalies contextuelles')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

print(f"Anomalie hiver: {anomalous_series[winter_anomaly_idx]:.1f}¬∞C en {dates[winter_anomaly_idx]}")
print(f"Anomalie √©t√©: {anomalous_series[summer_anomaly_idx]:.1f}¬∞C en {dates[summer_anomaly_idx]}")
```

#### 3. Anomalies collectives

```python
# Simulation trafic r√©seau avec anomalie collective
hours = np.arange(0, 24*7, 0.5)  # Une semaine, mesures toutes les 30min
n_points = len(hours)

# Pattern normal : trafic plus √©lev√© en journ√©e
day_pattern = 50 + 30 * np.sin(2 * np.pi * (hours % 24) / 24) ** 2
week_pattern = day_pattern * (1 + 0.1 * np.sin(2 * np.pi * hours / (24*7)))
noise = np.random.normal(0, 5, n_points)
normal_traffic = np.maximum(week_pattern + noise, 0)

# Injection d'anomalie collective : pic coordonn√© (DDoS?)
attack_start = int(len(hours) * 0.6)  # Milieu de semaine
attack_duration = 20  # 10 heures
attack_traffic = normal_traffic.copy()
attack_traffic[attack_start:attack_start+attack_duration] *= 3  # Triple du trafic

plt.figure(figsize=(15, 8))

plt.subplot(2, 1, 1)
plt.plot(hours, normal_traffic, 'b-', alpha=0.7)
plt.ylabel('Trafic (Mbps)')
plt.title('Trafic r√©seau normal')
plt.grid(True)

plt.subplot(2, 1, 2)
plt.plot(hours, attack_traffic, 'g-', alpha=0.7, label='Trafic observ√©')
plt.axvspan(hours[attack_start], hours[attack_start+attack_duration-1], 
            alpha=0.3, color='red', label='Anomalie collective')
plt.ylabel('Trafic (Mbps)')
plt.xlabel('Heures depuis d√©but semaine')
plt.title('Trafic avec anomalie collective (potentielle attaque)')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

print(f"D√©but anomalie: heure {hours[attack_start]:.1f}")
print(f"Dur√©e anomalie: {attack_duration * 0.5} heures")
print(f"Trafic moyen normal: {np.mean(normal_traffic):.1f} Mbps")
print(f"Trafic moyen pendant attaque: {np.mean(attack_traffic[attack_start:attack_start+attack_duration]):.1f} Mbps")
```

---

## üå≥ Isolation Forest

### Principe fondamental

**Isolation Forest** se base sur l'id√©e que les **anomalies sont plus faciles √† isoler** que les points normaux.

```mermaid
graph TD
    A[Dataset] --> B[Construction for√™t<br/>d'arbres d'isolation]
    B --> C[Pour chaque point]
    C --> D[Calcul profondeur moyenne<br/>d'isolation]
    D --> E{Profondeur faible?}
    E -->|Oui| F[Point anormal<br/>Facile √† isoler]
    E -->|Non| G[Point normal<br/>Difficile √† isoler]
    
    style F fill:#ffcdd2
    style G fill:#c8e6c9
```

### Intuition

```python
from sklearn.ensemble import IsolationForest
from sklearn.datasets import make_blobs

# G√©n√©ration de donn√©es avec clusters
X_normal, _ = make_blobs(n_samples=200, centers=2, cluster_std=1.5, 
                        center_box=(-5, 5), random_state=42)

# Ajout d'anomalies
X_anomalies = np.array([[-8, -8], [8, 8], [-8, 8], [8, -8], [0, -10]])
X_combined = np.vstack([X_normal, X_anomalies])

plt.figure(figsize=(15, 5))

# 1. Donn√©es avec anomalies
plt.subplot(1, 3, 1)
plt.scatter(X_normal[:, 0], X_normal[:, 1], c='blue', alpha=0.6, label='Normal')
plt.scatter(X_anomalies[:, 0], X_anomalies[:, 1], c='red', s=100, 
           label='Anomalies', marker='x')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Donn√©es originales')
plt.legend()
plt.grid(True)

# 2. Isolation Forest
iso_forest = IsolationForest(contamination=0.1, random_state=42)
y_pred = iso_forest.fit_predict(X_combined)
scores = iso_forest.score_samples(X_combined)

plt.subplot(1, 3, 2)
# Points normaux (label=1)
normal_mask = y_pred == 1
plt.scatter(X_combined[normal_mask, 0], X_combined[normal_mask, 1], 
           c='blue', alpha=0.6, label='Pr√©diction: Normal')
# Points anormaux (label=-1)
anomaly_mask = y_pred == -1
plt.scatter(X_combined[anomaly_mask, 0], X_combined[anomaly_mask, 1], 
           c='red', s=100, label='Pr√©diction: Anomalie', marker='x')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Pr√©dictions Isolation Forest')
plt.legend()
plt.grid(True)

# 3. Scores d'anomalie
plt.subplot(1, 3, 3)
colors = ['red' if s < -0.05 else 'blue' for s in scores]
plt.scatter(X_combined[:, 0], X_combined[:, 1], c=scores, 
           cmap='coolwarm', s=60, alpha=0.8)
plt.colorbar(label='Score d\'anomalie')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Scores d\'anomalie\n(plus n√©gatif = plus anormal)')
plt.grid(True)

plt.tight_layout()
plt.show()

print("Scores d'anomalie (5 plus faibles):")
worst_indices = np.argsort(scores)[:5]
for i, idx in enumerate(worst_indices):
    print(f"{i+1}. Point {idx}: score={scores[idx]:.3f}, coordonn√©es={X_combined[idx]}")
```

### Algorithme d√©taill√©

```python
class IsolationTreeSimple:
    """Impl√©mentation simplifi√©e d'un arbre d'isolation"""
    
    def __init__(self, max_depth=10):
        self.max_depth = max_depth
        self.split_feature = None
        self.split_value = None
        self.left = None
        self.right = None
        self.size = 0
        self.depth = 0
    
    def fit(self, X, depth=0):
        """Construction de l'arbre d'isolation"""
        self.size = len(X)
        self.depth = depth
        
        # Condition d'arr√™t
        if depth >= self.max_depth or len(X) <= 1:
            return self
        
        # S√©lection al√©atoire d'une feature et d'une valeur de split
        n_features = X.shape[1]
        self.split_feature = np.random.randint(0, n_features)
        
        feature_values = X[:, self.split_feature]
        min_val, max_val = feature_values.min(), feature_values.max()
        
        if min_val == max_val:  # Toutes les valeurs identiques
            return self
        
        self.split_value = np.random.uniform(min_val, max_val)
        
        # Division des donn√©es
        left_mask = feature_values < self.split_value
        right_mask = ~left_mask
        
        if np.sum(left_mask) > 0:
            self.left = IsolationTreeSimple(self.max_depth)
            self.left.fit(X[left_mask], depth + 1)
        
        if np.sum(right_mask) > 0:
            self.right = IsolationTreeSimple(self.max_depth)
            self.right.fit(X[right_mask], depth + 1)
        
        return self
    
    def path_length(self, x):
        """Calcul de la profondeur d'isolation pour un point"""
        if self.split_feature is None:  # Feuille
            # Approximation pour la profondeur moyenne dans un n≈ìud de taille self.size
            if self.size <= 1:
                return 0
            return np.log2(self.size)  # Approximation BST
        
        if x[self.split_feature] < self.split_value:
            if self.left is not None:
                return 1 + self.left.path_length(x)
        else:
            if self.right is not None:
                return 1 + self.right.path_length(x)
        
        return 1  # Si branche manquante

class IsolationForestSimple:
    """Impl√©mentation simplifi√©e d'Isolation Forest"""
    
    def __init__(self, n_estimators=100, max_depth=10, contamination=0.1):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.contamination = contamination
        self.trees = []
    
    def fit(self, X):
        """Entra√Ænement de la for√™t d'isolation"""
        self.trees = []
        n_samples = len(X)
        
        for _ in range(self.n_estimators):
            # √âchantillonnage al√©atoire (bootstrap)
            sample_indices = np.random.choice(n_samples, 
                                            size=min(256, n_samples), 
                                            replace=False)
            sample = X[sample_indices]
            
            # Construction de l'arbre
            tree = IsolationTreeSimple(self.max_depth)
            tree.fit(sample)
            self.trees.append(tree)
        
        return self
    
    def score_samples(self, X):
        """Calcul des scores d'anomalie"""
        scores = []
        
        for x in X:
            # Profondeur moyenne d'isolation
            path_lengths = [tree.path_length(x) for tree in self.trees]
            avg_path_length = np.mean(path_lengths)
            
            # Normalisation (formule d'Isolation Forest)
            # c(n) = 2*H(n-1) - (2*(n-1)/n) o√π H est le nombre harmonique
            n = 256  # Taille d'√©chantillon standard
            c_n = 2 * (np.log(n - 1) + 0.5772156649) - (2 * (n - 1) / n)
            
            # Score d'anomalie
            score = 2 ** (-avg_path_length / c_n)
            scores.append(score)
        
        return np.array(scores)
    
    def predict(self, X):
        """Pr√©diction binaire (normal=1, anomalie=-1)"""
        scores = self.score_samples(X)
        threshold = np.percentile(scores, (1 - self.contamination) * 100)
        return np.where(scores > threshold, -1, 1)

# Test de notre impl√©mentation
print("=== Comparaison avec sklearn ===")

# Notre impl√©mentation
iso_custom = IsolationForestSimple(n_estimators=100, contamination=0.1)
iso_custom.fit(X_combined)
scores_custom = iso_custom.score_samples(X_combined)
pred_custom = iso_custom.predict(X_combined)

# Sklearn
iso_sklearn = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)
pred_sklearn = iso_sklearn.fit_predict(X_combined)
scores_sklearn = iso_sklearn.score_samples(X_combined)

# Comparaison
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(X_combined[:, 0], X_combined[:, 1], c=scores_custom, 
           cmap='coolwarm', alpha=0.8)
plt.colorbar(label='Score custom')
plt.title('Notre impl√©mentation')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

plt.subplot(1, 2, 2)
plt.scatter(X_combined[:, 0], X_combined[:, 1], c=scores_sklearn, 
           cmap='coolwarm', alpha=0.8)
plt.colorbar(label='Score sklearn')
plt.title('Sklearn IsolationForest')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')

plt.tight_layout()
plt.show()

print(f"Anomalies d√©tect√©es - Custom: {np.sum(pred_custom == -1)}")
print(f"Anomalies d√©tect√©es - Sklearn: {np.sum(pred_sklearn == -1)}")
```

---

## üéØ One-Class SVM

### Principe

**One-Class SVM** cr√©e une **fronti√®re** qui englobe la majorit√© des donn√©es normales et exclut les anomalies.

```mermaid
graph TD
    A[Donn√©es d'entra√Ænement<br/>classe normale uniquement] --> B[Mapping vers espace<br/>de features sup√©rieur]
    B --> C[Recherche hyperplan optimal]
    C --> D[S√©paration donn√©es normales<br/>de l'origine]
    D --> E[Fronti√®re de d√©cision]
    
    subgraph "Param√®tres cl√©s"
        F[nu: fraction d'anomalies<br/>attendues]
        G[gamma: largeur du kernel<br/>RBF]
    end
    
    style E fill:#c8e6c9
```

### Impl√©mentation et comparaison

```python
from sklearn.svm import OneClassSVM
from sklearn.preprocessing import StandardScaler

# Pr√©paration des donn√©es
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_combined)

# Comparaison de diff√©rents param√®tres
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# Param√®tres √† tester
nu_values = [0.05, 0.1, 0.2]
gamma_values = ['scale', 0.1, 1.0]

for i, nu in enumerate(nu_values):
    for j, gamma in enumerate(gamma_values):
        # One-Class SVM
        oc_svm = OneClassSVM(nu=nu, gamma=gamma, kernel='rbf')
        y_pred_svm = oc_svm.fit_predict(X_scaled)
        
        # Visualisation
        ax = axes[i, j] if len(axes.shape) > 1 else axes[j]
        
        # Points normaux
        normal_mask = y_pred_svm == 1
        ax.scatter(X_scaled[normal_mask, 0], X_scaled[normal_mask, 1], 
                  c='blue', alpha=0.6, label='Normal')
        
        # Points anormaux
        anomaly_mask = y_pred_svm == -1
        ax.scatter(X_scaled[anomaly_mask, 0], X_scaled[anomaly_mask, 1], 
                  c='red', s=100, label='Anomalie', marker='x')
        
        # Fronti√®re de d√©cision
        xx, yy = np.meshgrid(np.linspace(X_scaled[:, 0].min()-1, X_scaled[:, 0].max()+1, 50),
                            np.linspace(X_scaled[:, 1].min()-1, X_scaled[:, 1].max()+1, 50))
        Z = oc_svm.decision_function(np.c_[xx.ravel(), yy.ravel()])
        Z = Z.reshape(xx.shape)
        
        ax.contour(xx, yy, Z, levels=[0], linewidths=2, colors='green')
        ax.contourf(xx, yy, Z, levels=[0, Z.max()], alpha=0.2, colors=['green'])
        
        ax.set_title(f'nu={nu}, gamma={gamma}\nAnomalies: {np.sum(anomaly_mask)}')
        ax.set_xlabel('Feature 1 (scaled)')
        ax.set_ylabel('Feature 2 (scaled)')
        ax.legend()
        ax.grid(True)

plt.tight_layout()
plt.show()
```

### Comparaison Isolation Forest vs One-Class SVM

```python
def compare_anomaly_methods(X, contamination=0.1, title="Comparaison m√©thodes"):
    """Comparaison de diff√©rentes m√©thodes de d√©tection d'anomalies"""
    
    # Standardisation
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # M√©thodes √† comparer
    methods = {
        'Isolation Forest': IsolationForest(contamination=contamination, random_state=42),
        'One-Class SVM (RBF)': OneClassSVM(nu=contamination, gamma='scale'),
        'One-Class SVM (Linear)': OneClassSVM(nu=contamination, kernel='linear')
    }
    
    fig, axes = plt.subplots(1, len(methods), figsize=(15, 5))
    
    for i, (name, method) in enumerate(methods.items()):
        # Pr√©diction
        y_pred = method.fit_predict(X_scaled)
        
        # Visualisation
        normal_mask = y_pred == 1
        anomaly_mask = y_pred == -1
        
        axes[i].scatter(X_scaled[normal_mask, 0], X_scaled[normal_mask, 1], 
                       c='blue', alpha=0.6, label='Normal')
        axes[i].scatter(X_scaled[anomaly_mask, 0], X_scaled[anomaly_mask, 1], 
                       c='red', s=100, label='Anomalie', marker='x')
        
        # Fronti√®re de d√©cision pour SVM
        if 'SVM' in name:
            xx, yy = np.meshgrid(np.linspace(X_scaled[:, 0].min()-1, X_scaled[:, 0].max()+1, 50),
                                np.linspace(X_scaled[:, 1].min()-1, X_scaled[:, 1].max()+1, 50))
            Z = method.decision_function(np.c_[xx.ravel(), yy.ravel()])
            Z = Z.reshape(xx.shape)
            axes[i].contour(xx, yy, Z, levels=[0], linewidths=2, colors='green', alpha=0.7)
        
        axes[i].set_title(f'{name}\nAnomalies: {np.sum(anomaly_mask)} ({np.sum(anomaly_mask)/len(X)*100:.1f}%)')
        axes[i].set_xlabel('Feature 1 (scaled)')
        axes[i].set_ylabel('Feature 2 (scaled)')
        axes[i].legend()
        axes[i].grid(True)
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()
    
    return methods

# Application sur nos donn√©es
methods_results = compare_anomaly_methods(X_combined, contamination=0.05, 
                                        title="Comparaison sur donn√©es synth√©tiques")
```

---

## üìä √âvaluation des m√©thodes

### M√©triques d'√©valuation

```python
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve

def evaluate_anomaly_detection(y_true, y_pred, y_scores=None, method_name=""):
    """√âvaluation compl√®te d'une m√©thode de d√©tection d'anomalies"""
    
    # Conversion des labels (anomalie=1, normal=0 pour les m√©triques)
    y_true_bin = (y_true == -1).astype(int)
    y_pred_bin = (y_pred == -1).astype(int)
    
    print(f"=== √âvaluation {method_name} ===")
    
    # Matrice de confusion
    cm = confusion_matrix(y_true_bin, y_pred_bin)
    print("Matrice de confusion:")
    print("Pr√©diction    Normal  Anomalie")
    print(f"Vraie Normal    {cm[0,0]:4d}    {cm[0,1]:4d}")
    print(f"Vraie Anomalie  {cm[1,0]:4d}    {cm[1,1]:4d}")
    
    # M√©triques
    tn, fp, fn, tp = cm.ravel()
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    print(f"\nM√©triques:")
    print(f"Pr√©cision: {precision:.3f}")
    print(f"Rappel: {recall:.3f}")
    print(f"F1-Score: {f1:.3f}")
    
    # AUC si scores disponibles
    if y_scores is not None:
        # Inverser scores pour Isolation Forest (plus n√©gatif = plus anormal)
        if method_name == "Isolation Forest":
            y_scores = -y_scores
        
        try:
            auc = roc_auc_score(y_true_bin, y_scores)
            print(f"AUC-ROC: {auc:.3f}")
        except:
            print("AUC-ROC: Non calculable")
    
    return {
        'precision': precision,
        'recall': recall,
        'f1': f1,
        'confusion_matrix': cm
    }

# Cr√©ation d'un dataset avec labels connus
np.random.seed(42)
X_eval_normal = np.random.multivariate_normal([0, 0], [[1, 0.3], [0.3, 1]], 200)
X_eval_anomalies = np.random.multivariate_normal([4, 4], [[0.5, 0], [0, 0.5]], 20)
X_eval = np.vstack([X_eval_normal, X_eval_anomalies])
y_eval_true = np.array([1] * 200 + [-1] * 20)  # 1=normal, -1=anomalie

# Test des m√©thodes
scaler = StandardScaler()
X_eval_scaled = scaler.fit_transform(X_eval)

# Isolation Forest
iso_forest = IsolationForest(contamination=0.1, random_state=42)
y_pred_iso = iso_forest.fit_predict(X_eval_scaled)
scores_iso = iso_forest.score_samples(X_eval_scaled)

# One-Class SVM
oc_svm = OneClassSVM(nu=0.1, gamma='scale')
y_pred_svm = oc_svm.fit_predict(X_eval_scaled)
scores_svm = oc_svm.decision_function(X_eval_scaled)

# √âvaluations
results_iso = evaluate_anomaly_detection(y_eval_true, y_pred_iso, scores_iso, "Isolation Forest")
print()
results_svm = evaluate_anomaly_detection(y_eval_true, y_pred_svm, scores_svm, "One-Class SVM")
```

### Courbes ROC et Precision-Recall

```python
from sklearn.metrics import roc_curve, auc

def plot_roc_pr_curves(y_true, methods_scores, methods_names):
    """Visualisation des courbes ROC et Precision-Recall"""
    
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Conversion labels
    y_true_bin = (y_true == -1).astype(int)
    
    for i, (scores, name) in enumerate(zip(methods_scores, methods_names)):
        # Ajustement des scores pour Isolation Forest
        if "Isolation" in name:
            scores = -scores
        
        # ROC Curve
        fpr, tpr, _ = roc_curve(y_true_bin, scores)
        roc_auc = auc(fpr, tpr)
        
        axes[0].plot(fpr, tpr, linewidth=2, 
                    label=f'{name} (AUC = {roc_auc:.3f})')
        
        # Precision-Recall Curve
        precision, recall, _ = precision_recall_curve(y_true_bin, scores)
        pr_auc = auc(recall, precision)
        
        axes[1].plot(recall, precision, linewidth=2,
                    label=f'{name} (AUC = {pr_auc:.3f})')
    
    # ROC plot
    axes[0].plot([0, 1], [0, 1], 'k--', alpha=0.5)
    axes[0].set_xlabel('Taux de Faux Positifs')
    axes[0].set_ylabel('Taux de Vrais Positifs')
    axes[0].set_title('Courbe ROC')
    axes[0].legend()
    axes[0].grid(True)
    
    # PR plot
    baseline = np.sum(y_true_bin) / len(y_true_bin)
    axes[1].axhline(y=baseline, color='k', linestyle='--', alpha=0.5, 
                   label=f'Baseline ({baseline:.3f})')
    axes[1].set_xlabel('Rappel')
    axes[1].set_ylabel('Pr√©cision')
    axes[1].set_title('Courbe Pr√©cision-Rappel')
    axes[1].legend()
    axes[1].grid(True)
    
    plt.tight_layout()
    plt.show()

# Application
plot_roc_pr_curves(y_eval_true, 
                  [scores_iso, scores_svm], 
                  ["Isolation Forest", "One-Class SVM"])
```

---

## üõ°Ô∏è Applications pratiques

### 1. D√©tection de fraude dans les transactions

```python
# Simulation de donn√©es de transactions
np.random.seed(42)

def generate_transaction_data(n_normal=1000, n_fraud=50):
    """G√©n√©ration de donn√©es simul√©es de transactions"""
    
    # Transactions normales
    normal_amounts = np.random.lognormal(mean=3, sigma=1, size=n_normal)
    normal_times = np.random.uniform(6, 22, size=n_normal)  # Heures normales
    normal_locations = np.random.multivariate_normal([48.8566, 2.3522], 
                                                    [[0.01, 0], [0, 0.01]], n_normal)  # Paris
    
    # Transactions frauduleuses
    fraud_amounts = np.concatenate([
        np.random.uniform(5000, 20000, n_fraud//2),  # Montants tr√®s √©lev√©s
        np.random.uniform(0.01, 1, n_fraud//2)       # Montants tr√®s faibles (tests)
    ])
    fraud_times = np.random.uniform(0, 24, size=n_fraud)  # N'importe quelle heure
    fraud_locations = np.random.uniform([40, -5], [55, 15], size=(n_fraud, 2))  # Locations vari√©es
    
    # Combinaison
    amounts = np.concatenate([normal_amounts, fraud_amounts])
    times = np.concatenate([normal_times, fraud_times])
    locations = np.vstack([normal_locations, fraud_locations])
    
    # Features
    X = np.column_stack([
        amounts,
        times,
        locations[:, 0],  # latitude
        locations[:, 1],  # longitude
    ])
    
    y = np.array([1] * n_normal + [-1] * n_fraud)
    
    return X, y

# G√©n√©ration des donn√©es
X_transactions, y_transactions = generate_transaction_data()

feature_names = ['Montant (‚Ç¨)', 'Heure', 'Latitude', 'Longitude']

# Visualisation des donn√©es
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

for i, feature in enumerate(feature_names):
    ax = axes[i//2, i%2]
    
    normal_data = X_transactions[y_transactions == 1, i]
    fraud_data = X_transactions[y_transactions == -1, i]
    
    ax.hist(normal_data, bins=30, alpha=0.7, label='Normal', density=True)
    ax.hist(fraud_data, bins=20, alpha=0.7, label='Fraude', density=True)
    ax.set_xlabel(feature)
    ax.set_ylabel('Densit√©')
    ax.set_title(f'Distribution {feature}')
    ax.legend()
    ax.grid(True)

plt.tight_layout()
plt.show()

# Application des m√©thodes de d√©tection
scaler = StandardScaler()
X_transactions_scaled = scaler.fit_transform(X_transactions)

# Isolation Forest pour fraude
iso_fraud = IsolationForest(contamination=0.05, random_state=42)
y_pred_fraud_iso = iso_fraud.fit_predict(X_transactions_scaled)
scores_fraud_iso = iso_fraud.score_samples(X_transactions_scaled)

# One-Class SVM pour fraude
svm_fraud = OneClassSVM(nu=0.05, gamma='scale')
y_pred_fraud_svm = svm_fraud.fit_predict(X_transactions_scaled)
scores_fraud_svm = svm_fraud.decision_function(X_transactions_scaled)

# √âvaluation
print("=== D√âTECTION DE FRAUDE ===")
eval_fraud_iso = evaluate_anomaly_detection(y_transactions, y_pred_fraud_iso, 
                                           scores_fraud_iso, "Isolation Forest")
print()
eval_fraud_svm = evaluate_anomaly_detection(y_transactions, y_pred_fraud_svm, 
                                           scores_fraud_svm, "One-Class SVM")

# Analyse des transactions d√©tect√©es comme frauduleuses
fraud_detected_iso = X_transactions[y_pred_fraud_iso == -1]
print(f"\n=== ANALYSE DES FRAUDES D√âTECT√âES (Isolation Forest) ===")
print(f"Nombre total d√©tect√©: {len(fraud_detected_iso)}")
print(f"Montant moyen d√©tect√©: {np.mean(fraud_detected_iso[:, 0]):.2f}‚Ç¨")
print(f"Montant m√©dian d√©tect√©: {np.median(fraud_detected_iso[:, 0]):.2f}‚Ç¨")
print(f"Heure moyenne d√©tect√©e: {np.mean(fraud_detected_iso[:, 1]):.1f}h")
```

### 2. Surveillance de performance syst√®me

```python
def generate_system_metrics(n_days=30):
    """G√©n√©ration de m√©triques syst√®me sur n_days jours"""
    
    hours = np.arange(0, 24 * n_days, 0.5)  # Mesures toutes les 30min
    n_points = len(hours)
    
    # Patterns normaux
    # CPU : plus √©lev√© en journ√©e
    cpu_base = 30 + 20 * np.sin(2 * np.pi * (hours % 24) / 24) ** 2
    cpu_noise = np.random.normal(0, 5, n_points)
    cpu_usage = np.clip(cpu_base + cpu_noise, 0, 100)
    
    # M√©moire : croissance lente + cycles
    memory_trend = 40 + 0.5 * hours / 24  # L√©g√®re croissance
    memory_cycle = 10 * np.sin(2 * np.pi * hours / 24)
    memory_noise = np.random.normal(0, 3, n_points)
    memory_usage = np.clip(memory_trend + memory_cycle + memory_noise, 0, 100)
    
    # R√©seau : bursts al√©atoires
    network_base = np.random.exponential(10, n_points)
    network_usage = np.clip(network_base, 0, 100)
    
    # Disk I/O : corr√©l√© au CPU
    disk_base = 0.7 * cpu_usage + np.random.normal(0, 5, n_points)
    disk_usage = np.clip(disk_base, 0, 100)
    
    # Injection d'anomalies
    # Pic CPU prolong√© (incident)
    incident_start = len(hours) // 2
    incident_duration = 48  # 24 heures
    cpu_usage[incident_start:incident_start+incident_duration] += 40
    cpu_usage = np.clip(cpu_usage, 0, 100)
    
    # Fuite m√©moire
    leak_start = int(len(hours) * 0.7)
    memory_usage[leak_start:] += np.linspace(0, 30, len(hours) - leak_start)
    memory_usage = np.clip(memory_usage, 0, 100)
    
    # Pic r√©seau soudain
    network_spike = int(len(hours) * 0.8)
    network_usage[network_spike:network_spike+4] = 95
    
    X = np.column_stack([cpu_usage, memory_usage, network_usage, disk_usage])
    timestamps = hours
    
    return X, timestamps

# G√©n√©ration et visualisation
X_system, timestamps = generate_system_metrics()
feature_names = ['CPU (%)', 'Memory (%)', 'Network (%)', 'Disk I/O (%)']

plt.figure(figsize=(15, 12))

for i, feature in enumerate(feature_names):
    plt.subplot(4, 1, i+1)
    plt.plot(timestamps, X_system[:, i], alpha=0.8)
    plt.ylabel(feature)
    plt.title(f'M√©triques syst√®me - {feature}')
    plt.grid(True)
    
    if i == len(feature_names) - 1:
        plt.xlabel('Heures depuis d√©but surveillance')

plt.tight_layout()
plt.show()

# D√©tection d'anomalies syst√®me
scaler_system = StandardScaler()
X_system_scaled = scaler_system.fit_transform(X_system)

iso_system = IsolationForest(contamination=0.05, random_state=42)
y_pred_system = iso_system.fit_predict(X_system_scaled)
scores_system = iso_system.score_samples(X_system_scaled)

# Visualisation des anomalies d√©tect√©es
anomaly_indices = np.where(y_pred_system == -1)[0]
anomaly_times = timestamps[anomaly_indices]

plt.figure(figsize=(15, 12))

for i, feature in enumerate(feature_names):
    plt.subplot(4, 1, i+1)
    plt.plot(timestamps, X_system[:, i], alpha=0.8, label='Normal')
    plt.scatter(anomaly_times, X_system[anomaly_indices, i], 
               color='red', s=20, label='Anomalie d√©tect√©e', zorder=5)
    plt.ylabel(feature)
    plt.title(f'{feature} - Anomalies d√©tect√©es')
    plt.legend()
    plt.grid(True)
    
    if i == len(feature_names) - 1:
        plt.xlabel('Heures depuis d√©but surveillance')

plt.tight_layout()
plt.show()

print(f"=== SURVEILLANCE SYST√àME ===")
print(f"Nombre d'anomalies d√©tect√©es: {len(anomaly_indices)}")
print(f"Pourcentage du temps anormal: {len(anomaly_indices)/len(X_system)*100:.2f}%")

# Analyse des p√©riodes anormales
if len(anomaly_indices) > 0:
    print(f"\nPremi√®res anomalies d√©tect√©es:")
    for i, idx in enumerate(anomaly_indices[:5]):
        print(f"Heure {timestamps[idx]:.1f}: CPU={X_system[idx,0]:.1f}%, "
              f"Mem={X_system[idx,1]:.1f}%, Net={X_system[idx,2]:.1f}%, "
              f"Disk={X_system[idx,3]:.1f}%")
```

---

## üîÑ Techniques avanc√©es

### Local Outlier Factor (LOF)

```python
from sklearn.neighbors import LocalOutlierFactor

def compare_with_lof(X, contamination=0.1):
    """Comparaison avec Local Outlier Factor"""
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # M√©thodes
    methods = {
        'Isolation Forest': IsolationForest(contamination=contamination, random_state=42),
        'One-Class SVM': OneClassSVM(nu=contamination, gamma='scale'),
        'LOF': LocalOutlierFactor(contamination=contamination, novelty=False)
    }
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    for i, (name, method) in enumerate(methods.items()):
        if name == 'LOF':
            y_pred = method.fit_predict(X_scaled)
            scores = method.negative_outlier_factor_
        else:
            y_pred = method.fit_predict(X_scaled)
            scores = method.score_samples(X_scaled) if hasattr(method, 'score_samples') else method.decision_function(X_scaled)
        
        # Visualisation
        normal_mask = y_pred == 1
        anomaly_mask = y_pred == -1
        
        axes[i].scatter(X_scaled[normal_mask, 0], X_scaled[normal_mask, 1], 
                       c='blue', alpha=0.6, s=30, label='Normal')
        axes[i].scatter(X_scaled[anomaly_mask, 0], X_scaled[anomaly_mask, 1], 
                       c='red', s=100, label='Anomalie', marker='x')
        
        axes[i].set_title(f'{name}\nAnomalies: {np.sum(anomaly_mask)}')
        axes[i].set_xlabel('Feature 1 (scaled)')
        axes[i].set_ylabel('Feature 2 (scaled)')
        axes[i].legend()
        axes[i].grid(True)
    
    plt.tight_layout()
    plt.show()

compare_with_lof(X_combined, contamination=0.1)
```

### Ensemble de d√©tecteurs

```python
def ensemble_anomaly_detection(X, contamination=0.1, voting='soft'):
    """Ensemble de m√©thodes de d√©tection d'anomalies"""
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # Diff√©rents d√©tecteurs
    detectors = {
        'iso_forest_1': IsolationForest(contamination=contamination, 
                                       n_estimators=50, random_state=42),
        'iso_forest_2': IsolationForest(contamination=contamination, 
                                       n_estimators=100, max_samples=0.8, random_state=123),
        'svm_rbf': OneClassSVM(nu=contamination, gamma='scale'),
        'svm_poly': OneClassSVM(nu=contamination, kernel='poly', degree=3),
    }
    
    # Pr√©dictions
    predictions = {}
    scores_all = {}
    
    for name, detector in detectors.items():
        y_pred = detector.fit_predict(X_scaled)
        predictions[name] = y_pred
        
        # Scores
        if hasattr(detector, 'score_samples'):
            scores = detector.score_samples(X_scaled)
        else:
            scores = detector.decision_function(X_scaled)
        scores_all[name] = scores
    
    # Voting
    if voting == 'hard':
        # Vote majoritaire
        pred_matrix = np.array(list(predictions.values())).T
        ensemble_pred = np.array([
            1 if np.sum(row == 1) > len(detectors) / 2 else -1 
            for row in pred_matrix
        ])
    else:  # soft voting
        # Moyenne des scores normalis√©s
        scores_normalized = []
        for name, scores in scores_all.items():
            # Normalisation min-max
            scores_norm = (scores - scores.min()) / (scores.max() - scores.min())
            scores_normalized.append(scores_norm)
        
        ensemble_scores = np.mean(scores_normalized, axis=0)
        threshold = np.percentile(ensemble_scores, (1 - contamination) * 100)
        ensemble_pred = np.where(ensemble_scores >= threshold, 1, -1)
    
    return ensemble_pred, predictions, scores_all

# Test de l'ensemble
ensemble_pred, individual_preds, individual_scores = ensemble_anomaly_detection(X_combined)

# Visualisation
fig, axes = plt.subplots(2, 3, figsize=(18, 12))

# M√©thodes individuelles
for i, (name, pred) in enumerate(individual_preds.items()):
    row, col = i // 3, i % 3
    
    normal_mask = pred == 1
    anomaly_mask = pred == -1
    
    axes[row, col].scatter(X_combined[normal_mask, 0], X_combined[normal_mask, 1], 
                          c='blue', alpha=0.6, s=30, label='Normal')
    axes[row, col].scatter(X_combined[anomaly_mask, 0], X_combined[anomaly_mask, 1], 
                          c='red', s=100, label='Anomalie', marker='x')
    
    axes[row, col].set_title(f'{name}\nAnomalies: {np.sum(anomaly_mask)}')
    axes[row, col].legend()
    axes[row, col].grid(True)

# Ensemble
normal_mask_ensemble = ensemble_pred == 1
anomaly_mask_ensemble = ensemble_pred == -1

axes[1, 2].scatter(X_combined[normal_mask_ensemble, 0], X_combined[normal_mask_ensemble, 1], 
                  c='blue', alpha=0.6, s=30, label='Normal')
axes[1, 2].scatter(X_combined[anomaly_mask_ensemble, 0], X_combined[anomaly_mask_ensemble, 1], 
                  c='red', s=100, label='Anomalie', marker='x')

axes[1, 2].set_title(f'Ensemble (Soft Voting)\nAnomalies: {np.sum(anomaly_mask_ensemble)}')
axes[1, 2].legend()
axes[1, 2].grid(True)

plt.tight_layout()
plt.show()

print(f"=== R√âSULTATS ENSEMBLE ===")
for name, pred in individual_preds.items():
    print(f"{name}: {np.sum(pred == -1)} anomalies")
print(f"Ensemble: {np.sum(ensemble_pred == -1)} anomalies")
```

---

## üéØ Guide de choix de m√©thode

### Tableau de comparaison

| Crit√®re | Isolation Forest | One-Class SVM | LOF |
|---------|------------------|---------------|-----|
| **Type de donn√©es** | Num√©riques | Num√©riques | Num√©riques |
| **Scalabilit√©** | ‚úÖ Excellente | ‚ö†Ô∏è Limit√©e | ‚ö†Ô∏è Limit√©e |
| **Temps d'entra√Ænement** | üöÄ Rapide | üêå Lent | üêå Tr√®s lent |
| **M√©moire** | ‚úÖ Faible | ‚ö†Ô∏è Moyenne | ‚ùå √âlev√©e |
| **Interpr√©tabilit√©** | ‚úÖ Bonne | ‚ö†Ô∏è Limit√©e | ‚úÖ Bonne |
| **Hyperparam√®tres** | üéØ Peu | üéõÔ∏è Plusieurs | üéõÔ∏è Plusieurs |
| **Robustesse au bruit** | ‚úÖ Bonne | ‚ö†Ô∏è Sensible | ‚ö†Ô∏è Sensible |
| **Donn√©es haute dimension** | ‚úÖ Adapt√© | ‚ùå Difficile | ‚ùå Curse of dimensionality |

### Arbre de d√©cision

```mermaid
flowchart TD
    A[D√©tection d'anomalies] --> B{Taille du dataset?}
    
    B --> C[Grand > 10k points]
    B --> D[Moyen/Petit < 10k points]
    
    C --> E[Isolation Forest]
    
    D --> F{Structure des donn√©es?}
    
    F --> G[Clusters denses] --> H[LOF ou DBSCAN]
    F --> I[Fronti√®re complexe] --> J[One-Class SVM RBF]
    F --> K[Fronti√®re simple] --> L[One-Class SVM Linear]
    
    style E fill:#c8e6c9
    style H fill:#c8e6c9
    style J fill:#c8e6c9
    style L fill:#c8e6c9
```

### Recommandations par domaine

| Domaine d'application | M√©thode recommand√©e | Raison |
|----------------------|-------------------|---------|
| **Fraude bancaire** | Isolation Forest | Gros volumes, temps r√©el |
| **Cybers√©curit√©** | Ensemble methods | Robustesse critique |
| **IoT/Capteurs** | Isolation Forest | Streaming, scalabilit√© |
| **Images m√©dicales** | One-Class SVM | Pr√©cision requise |
| **Maintenance pr√©dictive** | LOF + domain knowledge | Patterns locaux |
| **Qualit√© industrielle** | Statistical methods + ML | Seuils connus |

---

## üéØ R√©capitulatif

**Points cl√©s √† retenir :**

### Isolation Forest
- **Principe** : Isolation plus facile pour anomalies
- **Avantages** : Rapide, scalable, peu d'hyperparam√®tres
- **Inconv√©nients** : Suppose isolation facile
- **Usage** : D√©tection g√©n√©rale, gros datasets

### One-Class SVM
- **Principe** : Fronti√®re englobant donn√©es normales
- **Avantages** : Fronti√®res complexes, kernel trick
- **Inconv√©nients** : Lent, sensible aux param√®tres
- **Usage** : Fronti√®res non-lin√©aires, petits datasets

### LOF (Local Outlier Factor)
- **Principe** : Densit√© locale comparative
- **Avantages** : D√©tecte anomalies dans clusters
- **Inconv√©nients** : Tr√®s lent, m√©moire importante
- **Usage** : Structures locales complexes

### Bonnes pratiques
1. **Commencer par Isolation Forest** : Simple et efficace
2. **Normaliser les donn√©es** : Crucial pour SVM et LOF
3. **Validation avec domaine expert** : Les anomalies doivent faire sens
4. **Surveiller d√©rive** : Recalibrer r√©guli√®rement
5. **Ensemble methods** : Pour applications critiques

### √âvaluation
1. **M√©triques appropri√©es** : Pr√©cision/Rappel > Accuracy
2. **Validation temporelle** : Train/test chronologique
3. **Co√ªt des erreurs** : Faux positifs vs faux n√©gatifs
4. **Feedback loop** : Am√©lioration continue

---

## üîó Pour aller plus loin

- **Deep Learning** : Autoencoders pour anomalies complexes
- **Time Series** : LSTM, Prophet pour donn√©es temporelles  
- **Streaming** : Algorithmes incr√©mentaux en temps r√©el
- **Explainabilit√©** : SHAP pour comprendre les d√©tections