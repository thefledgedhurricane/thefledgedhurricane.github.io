---
title: "Introduction aux r√©seaux de neurones : Perceptron et MLP"
description: "D√©couvrez les fondements du deep learning avec les perceptrons et les r√©seaux de neurones multicouches"
difficulty: "intermediate"  
estimatedTime: "45 minutes"
keywords: ["neural networks", "perceptron", "MLP", "backpropagation", "deep learning", "activation"]
---

# Introduction aux r√©seaux de neurones : Perceptron et MLP

## üéØ Objectifs d'apprentissage

√Ä la fin de cette le√ßon, vous serez capable de :
- ‚úÖ Comprendre le fonctionnement d'un neurone artificiel
- ‚úÖ Impl√©menter un perceptron from scratch
- ‚úÖ Construire et entra√Æner un r√©seau multicouches (MLP)
- ‚úÖ Ma√Ætriser l'algorithme de r√©tropropagation

---

## üß† Le neurone artificiel

### Inspiration biologique

```mermaid
graph LR
    subgraph "Neurone biologique"
        A[Dendrites] --> B[Corps cellulaire]
        B --> C[Axone]
        C --> D[Synapses]
    end
    
    subgraph "Neurone artificiel"
        E[Entr√©es x‚ÇÅ,x‚ÇÇ,...] --> F[Fonction d'agr√©gation Œ£]
        F --> G[Fonction d'activation œÜ]
        G --> H[Sortie y]
    end
    
    A -.-> E
    B -.-> F
    C -.-> G
    D -.-> H
```

### Mod√®le math√©matique

Un **neurone artificiel** calcule une combinaison lin√©aire de ses entr√©es, puis applique une **fonction d'activation** :

$$y = \phi\left(\sum_{i=1}^{n} w_i x_i + b\right) = \phi(w^T x + b)$$

O√π :
- $x_i$ : entr√©es
- $w_i$ : poids synaptiques  
- $b$ : biais (bias)
- $\phi$ : fonction d'activation

### Impl√©mentation d'un neurone

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification, make_circles
from sklearn.preprocessing import StandardScaler

class Neurone:
    """Impl√©mentation d'un neurone artificiel"""
    
    def __init__(self, n_inputs):
        # Initialisation al√©atoire des poids
        self.weights = np.random.normal(0, 0.1, n_inputs)
        self.bias = np.random.normal(0, 0.1)
    
    def sigmoid(self, z):
        """Fonction d'activation sigmo√Øde"""
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clipping pour stabilit√©
    
    def forward(self, x):
        """Propagation avant"""
        z = np.dot(x, self.weights) + self.bias
        return self.sigmoid(z)
    
    def predict(self, x, threshold=0.5):
        """Pr√©diction binaire"""
        return (self.forward(x) > threshold).astype(int)

# Test d'un neurone
neurone = Neurone(n_inputs=2)
print(f"Poids initiaux: {neurone.weights}")
print(f"Biais initial: {neurone.bias:.3f}")

# Test sur quelques points
test_points = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
for point in test_points:
    output = neurone.forward(point)
    prediction = neurone.predict(point)
    print(f"Point {point}: sortie={output:.3f}, pr√©diction={prediction}")
```

### Fonctions d'activation

```python
def plot_activation_functions():
    """Visualisation des principales fonctions d'activation"""
    
    x = np.linspace(-5, 5, 1000)
    
    # D√©finition des fonctions
    def sigmoid(x):
        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))
    
    def tanh(x):
        return np.tanh(x)
    
    def relu(x):
        return np.maximum(0, x)
    
    def leaky_relu(x, alpha=0.01):
        return np.where(x > 0, x, alpha * x)
    
    # Visualisation
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    axes[0, 0].plot(x, sigmoid(x), 'b-', linewidth=2)
    axes[0, 0].set_title('Sigmoid: œÉ(x) = 1/(1+e‚ÅªÀ£)')
    axes[0, 0].set_ylabel('œÉ(x)')
    axes[0, 0].grid(True)
    axes[0, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)
    axes[0, 0].axvline(x=0, color='k', linestyle='-', alpha=0.3)
    
    axes[0, 1].plot(x, tanh(x), 'r-', linewidth=2)
    axes[0, 1].set_title('Tanh: tanh(x) = (eÀ£-e‚ÅªÀ£)/(eÀ£+e‚ÅªÀ£)')
    axes[0, 1].set_ylabel('tanh(x)')
    axes[0, 1].grid(True)
    axes[0, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)
    axes[0, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)
    
    axes[1, 0].plot(x, relu(x), 'g-', linewidth=2)
    axes[1, 0].set_title('ReLU: max(0,x)')
    axes[1, 0].set_xlabel('x')
    axes[1, 0].set_ylabel('ReLU(x)')
    axes[1, 0].grid(True)
    axes[1, 0].axhline(y=0, color='k', linestyle='-', alpha=0.3)
    axes[1, 0].axvline(x=0, color='k', linestyle='-', alpha=0.3)
    
    axes[1, 1].plot(x, leaky_relu(x), 'm-', linewidth=2)
    axes[1, 1].set_title('Leaky ReLU: max(Œ±x,x) avec Œ±=0.01')
    axes[1, 1].set_xlabel('x')
    axes[1, 1].set_ylabel('Leaky ReLU(x)')
    axes[1, 1].grid(True)
    axes[1, 1].axhline(y=0, color='k', linestyle='-', alpha=0.3)
    axes[1, 1].axvline(x=0, color='k', linestyle='-', alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # Propri√©t√©s des fonctions
    print("Propri√©t√©s des fonctions d'activation:")
    print("="*50)
    print("Sigmoid:")
    print("  - Sortie: [0, 1]")
    print("  - D√©riv√©e: œÉ(x)(1-œÉ(x))")
    print("  - Probl√®me: vanishing gradient")
    print()
    print("Tanh:")
    print("  - Sortie: [-1, 1]")
    print("  - D√©riv√©e: 1-tanh¬≤(x)")
    print("  - Avantage: centr√© en 0")
    print()
    print("ReLU:")
    print("  - Sortie: [0, +‚àû]")
    print("  - D√©riv√©e: 1 si x>0, 0 sinon")
    print("  - Avantage: pas de vanishing gradient")
    print("  - Probl√®me: dying ReLU")
    print()
    print("Leaky ReLU:")
    print("  - Sortie: [-‚àû, +‚àû]")
    print("  - D√©riv√©e: 1 si x>0, Œ± sinon")
    print("  - Avantage: √©vite dying ReLU")

plot_activation_functions()
```

---

## ‚ö° Le Perceptron

### Algorithme historique

Le **Perceptron** (Rosenblatt, 1957) est le premier algorithme d'apprentissage pour neurones artificiels.

```mermaid
graph TD
    A[Initialisation poids al√©atoires] --> B[Pour chaque exemple]
    B --> C[Calcul pr√©diction ≈∑]
    C --> D{≈∑ = y?}
    D -->|Oui| E[Exemple suivant]
    D -->|Non| F[Mise √† jour poids]
    F --> G[w ‚Üê w + Œ∑(y-≈∑)x]
    G --> E
    E --> H{Tous exemples?}
    H -->|Non| B
    H -->|Oui| I{Convergence?}
    I -->|Non| B
    I -->|Oui| J[Arr√™t]
    
    style J fill:#c8e6c9
```

### Impl√©mentation du Perceptron

```python
class Perceptron:
    """Impl√©mentation du Perceptron classique"""
    
    def __init__(self, learning_rate=0.1, max_iter=1000):
        self.learning_rate = learning_rate
        self.max_iter = max_iter
        self.weights = None
        self.bias = None
        self.history = {'weights': [], 'bias': [], 'errors': []}
    
    def activation(self, z):
        """Fonction d'activation step (seuil)"""
        return np.where(z >= 0, 1, 0)
    
    def fit(self, X, y):
        """Entra√Ænement du perceptron"""
        n_samples, n_features = X.shape
        
        # Initialisation
        self.weights = np.random.normal(0, 0.1, n_features)
        self.bias = np.random.normal(0, 0.1)
        
        # Entra√Ænement
        for iteration in range(self.max_iter):
            errors = 0
            
            for i in range(n_samples):
                # Propagation avant
                z = np.dot(X[i], self.weights) + self.bias
                y_pred = self.activation(z)
                
                # Calcul erreur
                error = y[i] - y_pred
                
                if error != 0:
                    errors += 1
                    # Mise √† jour poids (r√®gle de Hebb modifi√©e)
                    self.weights += self.learning_rate * error * X[i]
                    self.bias += self.learning_rate * error
            
            # Sauvegarde pour historique
            self.history['weights'].append(self.weights.copy())
            self.history['bias'].append(self.bias)
            self.history['errors'].append(errors)
            
            # Crit√®re d'arr√™t
            if errors == 0:
                print(f"Convergence atteinte en {iteration + 1} it√©rations")
                break
        
        return self
    
    def predict(self, X):
        """Pr√©diction"""
        z = np.dot(X, self.weights) + self.bias
        return self.activation(z)
    
    def decision_function(self, X):
        """Fonction de d√©cision (avant activation)"""
        return np.dot(X, self.weights) + self.bias

# Test sur donn√©es lin√©airement s√©parables
np.random.seed(42)
X_linear, y_linear = make_classification(n_samples=100, n_features=2, 
                                        n_redundant=0, n_informative=2,
                                        n_clusters_per_class=1, random_state=42)

# Standardisation
scaler = StandardScaler()
X_linear_scaled = scaler.fit_transform(X_linear)

# Entra√Ænement
perceptron = Perceptron(learning_rate=0.1, max_iter=1000)
perceptron.fit(X_linear_scaled, y_linear)

# Visualisation des r√©sultats
def plot_perceptron_results(X, y, model, title="Perceptron"):
    """Visualisation des r√©sultats du perceptron"""
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
    
    # 1. Fronti√®re de d√©cision
    h = 0.02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                        np.arange(y_min, y_max, h))
    
    Z = model.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    axes[0].contourf(xx, yy, Z, levels=50, alpha=0.3, cmap='RdYlBu')
    axes[0].contour(xx, yy, Z, levels=[0], colors='black', linewidths=2)
    
    # Points de donn√©es
    scatter = axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='RdYlBu', edgecolors='black')
    axes[0].set_title(f'{title} - Fronti√®re de d√©cision')
    axes[0].set_xlabel('Feature 1')
    axes[0].set_ylabel('Feature 2')
    axes[0].grid(True)
    
    # 2. √âvolution des erreurs
    axes[1].plot(model.history['errors'], 'b-', linewidth=2)
    axes[1].set_title('Convergence - Nombre d\'erreurs')
    axes[1].set_xlabel('It√©ration')
    axes[1].set_ylabel('Nombre d\'erreurs')
    axes[1].grid(True)
    
    # 3. √âvolution des poids
    weights_history = np.array(model.history['weights'])
    bias_history = np.array(model.history['bias'])
    
    axes[2].plot(weights_history[:, 0], label='w1', linewidth=2)
    axes[2].plot(weights_history[:, 1], label='w2', linewidth=2)
    axes[2].plot(bias_history, label='bias', linewidth=2)
    axes[2].set_title('√âvolution des param√®tres')
    axes[2].set_xlabel('It√©ration')
    axes[2].set_ylabel('Valeur')
    axes[2].legend()
    axes[2].grid(True)
    
    plt.tight_layout()
    plt.show()

plot_perceptron_results(X_linear_scaled, y_linear, perceptron)

# √âvaluation
y_pred = perceptron.predict(X_linear_scaled)
accuracy = np.mean(y_pred == y_linear)
print(f"Pr√©cision: {accuracy:.3f}")
print(f"Poids finaux: {perceptron.weights}")
print(f"Biais final: {perceptron.bias:.3f}")
```

### Limitations du Perceptron

#### Probl√®me XOR

```python
# Le perceptron ne peut pas r√©soudre XOR (non lin√©airement s√©parable)
X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y_xor = np.array([0, 1, 1, 0])  # XOR

print("Probl√®me XOR:")
print("Entr√©es | Sortie attendue")
print("x1  x2  | y")
print("-" * 15)
for i in range(len(X_xor)):
    print(f"{X_xor[i, 0]:2d}  {X_xor[i, 1]:2d}  | {y_xor[i]}")

# Tentative d'entra√Ænement
perceptron_xor = Perceptron(learning_rate=0.1, max_iter=1000)
perceptron_xor.fit(X_xor, y_xor)

# Pr√©dictions
y_pred_xor = perceptron_xor.predict(X_xor)
print(f"\nPr√©dictions: {y_pred_xor}")
print(f"Attendu:     {y_xor}")
print(f"Pr√©cision: {np.mean(y_pred_xor == y_xor):.3f}")

# Visualisation du probl√®me
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
colors = ['red' if label == 0 else 'blue' for label in y_xor]
plt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)
for i, (x, y) in enumerate(X_xor):
    plt.annotate(f'({x},{y})\nXOR={y_xor[i]}', (x, y), 
                xytext=(10, 10), textcoords='offset points')
plt.title('Probl√®me XOR\n(Non lin√©airement s√©parable)')
plt.xlabel('x1')
plt.ylabel('x2')
plt.grid(True)
plt.xlim(-0.5, 1.5)
plt.ylim(-0.5, 1.5)

# Tentative de fronti√®re lin√©aire
plt.subplot(1, 2, 2)
plt.scatter(X_xor[:, 0], X_xor[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)

# Fronti√®re trouv√©e par le perceptron
if perceptron_xor.weights[1] != 0:  # √âviter division par z√©ro
    x_line = np.linspace(-0.5, 1.5, 100)
    y_line = -(perceptron_xor.weights[0] * x_line + perceptron_xor.bias) / perceptron_xor.weights[1]
    plt.plot(x_line, y_line, 'green', linewidth=2, label='Fronti√®re trouv√©e')

plt.title('Fronti√®re de d√©cision\n(Impossible avec une droite)')
plt.xlabel('x1')
plt.ylabel('x2')
plt.legend()
plt.grid(True)
plt.xlim(-0.5, 1.5)
plt.ylim(-0.5, 1.5)

plt.tight_layout()
plt.show()
```

---

## üåê R√©seaux multicouches (MLP)

### Architecture

Un **Multi-Layer Perceptron (MLP)** combine plusieurs couches de neurones pour r√©soudre des probl√®mes non-lin√©aires.

```mermaid
graph LR
    subgraph "Couche d'entr√©e"
        x1[x‚ÇÅ]
        x2[x‚ÇÇ]
        x3[x‚ÇÉ]
    end
    
    subgraph "Couche cach√©e 1"
        h1[h‚ÇÅ]
        h2[h‚ÇÇ]
        h3[h‚ÇÉ]
        h4[h‚ÇÑ]
    end
    
    subgraph "Couche cach√©e 2"
        h5[h‚ÇÖ]
        h6[h‚ÇÜ]
        h7[h‚Çá]
    end
    
    subgraph "Couche de sortie"
        y1[y‚ÇÅ]
        y2[y‚ÇÇ]
    end
    
    x1 --> h1
    x1 --> h2
    x1 --> h3
    x1 --> h4
    
    x2 --> h1
    x2 --> h2
    x2 --> h3
    x2 --> h4
    
    x3 --> h1
    x3 --> h2
    x3 --> h3
    x3 --> h4
    
    h1 --> h5
    h1 --> h6
    h1 --> h7
    
    h2 --> h5
    h2 --> h6
    h2 --> h7
    
    h3 --> h5
    h3 --> h6
    h3 --> h7
    
    h4 --> h5
    h4 --> h6
    h4 --> h7
    
    h5 --> y1
    h5 --> y2
    
    h6 --> y1
    h6 --> y2
    
    h7 --> y1
    h7 --> y2
```

### Impl√©mentation d'un MLP simple

```python
class MLPSimple:
    """Impl√©mentation simple d'un MLP √† une couche cach√©e"""
    
    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.learning_rate = learning_rate
        
        # Initialisation Xavier/Glorot
        self.W1 = np.random.normal(0, np.sqrt(2/(input_size + hidden_size)), 
                                  (input_size, hidden_size))
        self.b1 = np.zeros((1, hidden_size))
        
        self.W2 = np.random.normal(0, np.sqrt(2/(hidden_size + output_size)), 
                                  (hidden_size, output_size))
        self.b2 = np.zeros((1, output_size))
        
        # Historique pour analyse
        self.loss_history = []
        self.accuracy_history = []
    
    def sigmoid(self, z):
        """Fonction d'activation sigmo√Øde"""
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
    
    def sigmoid_derivative(self, z):
        """D√©riv√©e de la sigmo√Øde"""
        s = self.sigmoid(z)
        return s * (1 - s)
    
    def forward(self, X):
        """Propagation avant"""
        # Couche cach√©e
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        
        # Couche de sortie
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.sigmoid(self.z2)
        
        return self.a2
    
    def compute_loss(self, y_true, y_pred):
        """Calcul de la perte (entropie crois√©e binaire)"""
        m = y_true.shape[0]
        # √âviter log(0)
        y_pred = np.clip(y_pred, 1e-7, 1-1e-7)
        loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m
        return loss
    
    def backward(self, X, y_true, y_pred):
        """R√©tropropagation"""
        m = X.shape[0]
        
        # Gradient de la couche de sortie
        dz2 = y_pred - y_true
        dW2 = np.dot(self.a1.T, dz2) / m
        db2 = np.sum(dz2, axis=0, keepdims=True) / m
        
        # Gradient de la couche cach√©e
        da1 = np.dot(dz2, self.W2.T)
        dz1 = da1 * self.sigmoid_derivative(self.z1)
        dW1 = np.dot(X.T, dz1) / m
        db1 = np.sum(dz1, axis=0, keepdims=True) / m
        
        # Mise √† jour des param√®tres
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1
    
    def fit(self, X, y, epochs=1000, verbose=True):
        """Entra√Ænement du r√©seau"""
        for epoch in range(epochs):
            # Propagation avant
            y_pred = self.forward(X)
            
            # Calcul de la perte
            loss = self.compute_loss(y, y_pred)
            
            # R√©tropropagation
            self.backward(X, y, y_pred)
            
            # Calcul de la pr√©cision
            predictions = (y_pred > 0.5).astype(int)
            accuracy = np.mean(predictions == y)
            
            # Sauvegarde historique
            self.loss_history.append(loss)
            self.accuracy_history.append(accuracy)
            
            # Affichage
            if verbose and epoch % 100 == 0:
                print(f"√âpoque {epoch}: Perte = {loss:.4f}, Pr√©cision = {accuracy:.4f}")
    
    def predict(self, X):
        """Pr√©diction"""
        y_pred = self.forward(X)
        return (y_pred > 0.5).astype(int)
    
    def predict_proba(self, X):
        """Probabilit√© de pr√©diction"""
        return self.forward(X)

# Test sur le probl√®me XOR
print("=== R√©solution XOR avec MLP ===")

# Pr√©paration des donn√©es XOR
X_xor_expanded = X_xor.astype(float)
y_xor_expanded = y_xor.reshape(-1, 1).astype(float)

# Cr√©ation et entra√Ænement du MLP
mlp = MLPSimple(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)
mlp.fit(X_xor_expanded, y_xor_expanded, epochs=2000)

# Test des pr√©dictions
y_pred_mlp = mlp.predict(X_xor_expanded)
proba_mlp = mlp.predict_proba(X_xor_expanded)

print(f"\n=== R√©sultats XOR ===")
print("Entr√©es | Attendu | Pr√©dit | Probabilit√©")
print("-" * 45)
for i in range(len(X_xor)):
    print(f"[{X_xor[i, 0]}, {X_xor[i, 1]}]  |    {y_xor[i]:d}    |   {y_pred_mlp[i, 0]:d}   |   {proba_mlp[i, 0]:.3f}")

print(f"\nPr√©cision finale: {np.mean(y_pred_mlp.flatten() == y_xor):.3f}")
```

### Visualisation de l'apprentissage

```python
def visualize_mlp_learning(mlp, X, y, title="MLP Learning"):
    """Visualisation de l'apprentissage du MLP"""
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. √âvolution de la perte
    axes[0, 0].plot(mlp.loss_history, 'b-', linewidth=2)
    axes[0, 0].set_title('√âvolution de la perte')
    axes[0, 0].set_xlabel('√âpoque')
    axes[0, 0].set_ylabel('Perte')
    axes[0, 0].grid(True)
    axes[0, 0].set_yscale('log')
    
    # 2. √âvolution de la pr√©cision
    axes[0, 1].plot(mlp.accuracy_history, 'g-', linewidth=2)
    axes[0, 1].set_title('√âvolution de la pr√©cision')
    axes[0, 1].set_xlabel('√âpoque')
    axes[0, 1].set_ylabel('Pr√©cision')
    axes[0, 1].grid(True)
    axes[0, 1].set_ylim(0, 1.05)
    
    # 3. Fronti√®re de d√©cision (si 2D)
    if X.shape[1] == 2:
        h = 0.02
        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
        xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                            np.arange(y_min, y_max, h))
        
        grid_points = np.c_[xx.ravel(), yy.ravel()]
        Z = mlp.predict_proba(grid_points)
        Z = Z.reshape(xx.shape)
        
        contour = axes[1, 0].contourf(xx, yy, Z, levels=50, alpha=0.8, cmap='RdYlBu')
        axes[1, 0].contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)
        
        # Points de donn√©es
        colors = ['red' if label == 0 else 'blue' for label in y.flatten()]
        axes[1, 0].scatter(X[:, 0], X[:, 1], c=colors, s=200, edgecolors='black', linewidth=2)
        
        axes[1, 0].set_title('Fronti√®re de d√©cision')
        axes[1, 0].set_xlabel('x1')
        axes[1, 0].set_ylabel('x2')
        plt.colorbar(contour, ax=axes[1, 0])
    
    # 4. Distribution des poids
    all_weights = np.concatenate([mlp.W1.flatten(), mlp.W2.flatten()])
    axes[1, 1].hist(all_weights, bins=20, alpha=0.7, edgecolor='black')
    axes[1, 1].set_title('Distribution des poids finaux')
    axes[1, 1].set_xlabel('Valeur du poids')
    axes[1, 1].set_ylabel('Fr√©quence')
    axes[1, 1].grid(True)
    
    plt.suptitle(title)
    plt.tight_layout()
    plt.show()

# Visualisation pour XOR
visualize_mlp_learning(mlp, X_xor_expanded, y_xor_expanded, "MLP - R√©solution XOR")

# Analyse des poids appris
print("\n=== Analyse des poids appris ===")
print("Poids W1 (entr√©e -> couche cach√©e):")
print(mlp.W1)
print("\nBiais b1 (couche cach√©e):")
print(mlp.b1)
print("\nPoids W2 (couche cach√©e -> sortie):")
print(mlp.W2)
print("\nBiais b2 (sortie):")
print(mlp.b2)
```

---

## üîÑ Algorithme de r√©tropropagation

### Principe math√©matique

La **r√©tropropagation** calcule les gradients en propageant l'erreur de la sortie vers l'entr√©e.

```mermaid
graph TB
    A[Propagation avant] --> B[Calcul de la perte L]
    B --> C[Gradient sortie: ‚àÇL/‚àÇz_output]
    C --> D[Gradient poids sortie: ‚àÇL/‚àÇW_output]
    D --> E[Gradient activation: ‚àÇL/‚àÇa_hidden]
    E --> F[Gradient pre-activation: ‚àÇL/‚àÇz_hidden]
    F --> G[Gradient poids cach√©s: ‚àÇL/‚àÇW_hidden]
    G --> H[Mise √† jour param√®tres]
    
    style A fill:#e3f2fd
    style H fill:#c8e6c9
```

### Formules d√©taill√©es

Pour une couche $l$ avec :
- $z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$
- $a^{(l)} = \sigma(z^{(l)})$

Les gradients sont :
- $\frac{\partial L}{\partial W^{(l)}} = \frac{\partial L}{\partial z^{(l)}} \cdot (a^{(l-1)})^T$
- $\frac{\partial L}{\partial b^{(l)}} = \frac{\partial L}{\partial z^{(l)}}$
- $\frac{\partial L}{\partial a^{(l-1)}} = (W^{(l)})^T \cdot \frac{\partial L}{\partial z^{(l)}}$
- $\frac{\partial L}{\partial z^{(l-1)}} = \frac{\partial L}{\partial a^{(l-1)}} \odot \sigma'(z^{(l-1)})$

### Impl√©mentation d√©taill√©e avec calculs √©tape par √©tape

```python
class MLPDetailed:
    """MLP avec calculs d√©taill√©s pour comprendre la r√©tropropagation"""
    
    def __init__(self, layers_sizes, learning_rate=0.01):
        self.layers_sizes = layers_sizes
        self.learning_rate = learning_rate
        self.num_layers = len(layers_sizes)
        
        # Initialisation des poids
        self.weights = {}
        self.biases = {}
        
        for i in range(1, self.num_layers):
            self.weights[i] = np.random.normal(0, np.sqrt(2/layers_sizes[i-1]), 
                                             (layers_sizes[i-1], layers_sizes[i]))
            self.biases[i] = np.zeros((1, layers_sizes[i]))
        
        # Stockage pour analyse
        self.activations = {}
        self.z_values = {}
        self.gradients = {}
    
    def sigmoid(self, z):
        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))
    
    def sigmoid_derivative(self, z):
        s = self.sigmoid(z)
        return s * (1 - s)
    
    def forward_detailed(self, X, verbose=False):
        """Propagation avant avec d√©tails"""
        self.activations[0] = X
        
        if verbose:
            print("=== PROPAGATION AVANT ===")
            print(f"Entr√©e a^(0): shape {X.shape}")
            print(f"Premi√®re ligne: {X[0]}")
        
        for i in range(1, self.num_layers):
            # Calcul pre-activation
            self.z_values[i] = np.dot(self.activations[i-1], self.weights[i]) + self.biases[i]
            # Calcul activation
            self.activations[i] = self.sigmoid(self.z_values[i])
            
            if verbose:
                print(f"\nCouche {i}:")
                print(f"  Poids W^({i}): shape {self.weights[i].shape}")
                print(f"  z^({i}): shape {self.z_values[i].shape}")
                print(f"  a^({i}): shape {self.activations[i].shape}")
                print(f"  a^({i})[0]: {self.activations[i][0]}")
        
        return self.activations[self.num_layers - 1]
    
    def backward_detailed(self, y_true, verbose=False):
        """R√©tropropagation avec d√©tails"""
        m = y_true.shape[0]
        
        if verbose:
            print("\n=== R√âTROPROPAGATION ===")
        
        # Gradient de la couche de sortie
        output_layer = self.num_layers - 1
        y_pred = self.activations[output_layer]
        
        # dL/dz pour la derni√®re couche (entropie crois√©e + sigmo√Øde)
        self.gradients[f'dz{output_layer}'] = (y_pred - y_true) / m
        
        if verbose:
            print(f"Gradient sortie dz^({output_layer}): shape {self.gradients[f'dz{output_layer}'].shape}")
            print(f"Valeur moyenne: {np.mean(self.gradients[f'dz{output_layer}']):.4f}")
        
        # Propagation inverse couche par couche
        for i in range(output_layer, 0, -1):
            # Gradients des poids et biais
            self.gradients[f'dW{i}'] = np.dot(self.activations[i-1].T, self.gradients[f'dz{i}'])
            self.gradients[f'db{i}'] = np.sum(self.gradients[f'dz{i}'], axis=0, keepdims=True)
            
            if verbose:
                print(f"\nCouche {i}:")
                print(f"  dW^({i}): shape {self.gradients[f'dW{i}'].shape}")
                print(f"  db^({i}): shape {self.gradients[f'db{i}'].shape}")
            
            # Gradient pour la couche pr√©c√©dente (si elle existe)
            if i > 1:
                # dL/da^(i-1)
                self.gradients[f'da{i-1}'] = np.dot(self.gradients[f'dz{i}'], self.weights[i].T)
                # dL/dz^(i-1)
                self.gradients[f'dz{i-1}'] = (self.gradients[f'da{i-1}'] * 
                                            self.sigmoid_derivative(self.z_values[i-1]))
                
                if verbose:
                    print(f"  da^({i-1}): shape {self.gradients[f'da{i-1}'].shape}")
                    print(f"  dz^({i-1}): shape {self.gradients[f'dz{i-1}'].shape}")
    
    def update_parameters(self, verbose=False):
        """Mise √† jour des param√®tres"""
        if verbose:
            print("\n=== MISE √Ä JOUR PARAM√àTRES ===")
        
        for i in range(1, self.num_layers):
            old_w = self.weights[i].copy()
            old_b = self.biases[i].copy()
            
            self.weights[i] -= self.learning_rate * self.gradients[f'dW{i}']
            self.biases[i] -= self.learning_rate * self.gradients[f'db{i}']
            
            if verbose:
                weight_change = np.mean(np.abs(self.weights[i] - old_w))
                bias_change = np.mean(np.abs(self.biases[i] - old_b))
                print(f"Couche {i}: Œîpoids={weight_change:.6f}, Œîbiais={bias_change:.6f}")
    
    def train_step(self, X, y, verbose=False):
        """Une √©tape d'entra√Ænement compl√®te"""
        # Propagation avant
        y_pred = self.forward_detailed(X, verbose=verbose)
        
        # Calcul perte
        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)
        loss = -np.mean(y * np.log(y_pred_clipped) + (1 - y) * np.log(1 - y_pred_clipped))
        
        # R√©tropropagation
        self.backward_detailed(y, verbose=verbose)
        
        # Mise √† jour
        self.update_parameters(verbose=verbose)
        
        return loss, y_pred

# Test d√©taill√© sur XOR
print("=== ENTRA√éNEMENT D√âTAILL√â XOR ===")

mlp_detailed = MLPDetailed([2, 3, 1], learning_rate=1.0)
X_xor_float = X_xor.astype(float)
y_xor_float = y_xor.reshape(-1, 1).astype(float)

# Premi√®re it√©ration avec d√©tails
print("PREMI√àRE IT√âRATION:")
loss, pred = mlp_detailed.train_step(X_xor_float, y_xor_float, verbose=True)
print(f"\nPerte: {loss:.4f}")
print(f"Pr√©dictions: {pred.flatten()}")

# Quelques it√©rations suppl√©mentaires
print("\n" + "="*50)
print("ENTRA√éNEMENT COMPLET:")
for epoch in range(1000):
    loss, pred = mlp_detailed.train_step(X_xor_float, y_xor_float, verbose=False)
    if epoch % 200 == 0:
        accuracy = np.mean((pred > 0.5) == y_xor_float)
        print(f"√âpoque {epoch}: Perte = {loss:.4f}, Pr√©cision = {accuracy:.4f}")

# R√©sultats finaux
final_pred = (pred > 0.5).astype(int)
print(f"\n=== R√âSULTATS FINAUX ===")
print("x1 x2 | Attendu | Pr√©dit | Probabilit√©")
print("-" * 40)
for i in range(len(X_xor)):
    print(f"{X_xor[i, 0]:2d} {X_xor[i, 1]:2d} |    {y_xor[i]:d}    |   {final_pred[i, 0]:d}   |   {pred[i, 0]:.3f}")
```

---

## üéõÔ∏è Hyperparam√®tres et optimisation

### Taux d'apprentissage

```python
def compare_learning_rates():
    """Comparaison de diff√©rents taux d'apprentissage"""
    
    learning_rates = [0.1, 0.5, 1.0, 2.0, 5.0]
    colors = ['blue', 'green', 'red', 'orange', 'purple']
    
    plt.figure(figsize=(15, 10))
    
    for i, lr in enumerate(learning_rates):
        mlp = MLPSimple(2, 4, 1, learning_rate=lr)
        mlp.fit(X_xor_expanded, y_xor_expanded, epochs=1000, verbose=False)
        
        # Perte
        plt.subplot(2, 2, 1)
        plt.plot(mlp.loss_history, color=colors[i], label=f'LR = {lr}', linewidth=2)
        
        # Pr√©cision
        plt.subplot(2, 2, 2)
        plt.plot(mlp.accuracy_history, color=colors[i], label=f'LR = {lr}', linewidth=2)
        
        # Convergence (derni√®res 100 √©poques)
        final_loss = np.mean(mlp.loss_history[-100:])
        final_accuracy = np.mean(mlp.accuracy_history[-100:])
        
        plt.subplot(2, 2, 3)
        plt.bar(i, final_loss, color=colors[i], alpha=0.7)
        
        plt.subplot(2, 2, 4)
        plt.bar(i, final_accuracy, color=colors[i], alpha=0.7)
    
    plt.subplot(2, 2, 1)
    plt.title('√âvolution de la perte')
    plt.xlabel('√âpoque')
    plt.ylabel('Perte')
    plt.legend()
    plt.grid(True)
    plt.yscale('log')
    
    plt.subplot(2, 2, 2)
    plt.title('√âvolution de la pr√©cision')
    plt.xlabel('√âpoque')
    plt.ylabel('Pr√©cision')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(2, 2, 3)
    plt.title('Perte finale (moyenne 100 derni√®res √©poques)')
    plt.xlabel('Taux d\'apprentissage')
    plt.ylabel('Perte')
    plt.xticks(range(len(learning_rates)), learning_rates)
    plt.grid(True)
    
    plt.subplot(2, 2, 4)
    plt.title('Pr√©cision finale')
    plt.xlabel('Taux d\'apprentissage')
    plt.ylabel('Pr√©cision')
    plt.xticks(range(len(learning_rates)), learning_rates)
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

compare_learning_rates()
```

### Architecture du r√©seau

```python
def compare_architectures():
    """Comparaison de diff√©rentes architectures"""
    
    architectures = [
        (2, 2, 1),   # Minimal
        (2, 4, 1),   # Standard
        (2, 8, 1),   # Plus de neurones
        (2, 4, 2, 1), # Plus de couches
        (2, 6, 4, 1)  # Large puis √©troit
    ]
    
    names = ['Minimal (2-2-1)', 'Standard (2-4-1)', 'Large (2-8-1)', 
             'Profond (2-4-2-1)', 'Pyramidal (2-6-4-1)']
    
    colors = ['blue', 'green', 'red', 'orange', 'purple']
    
    plt.figure(figsize=(15, 10))
    
    results = []
    
    for i, (arch, name) in enumerate(zip(architectures, names)):
        if len(arch) == 3:  # MLP simple
            mlp = MLPSimple(arch[0], arch[1], arch[2], learning_rate=0.5)
            mlp.fit(X_xor_expanded, y_xor_expanded, epochs=1500, verbose=False)
            
            plt.subplot(2, 2, 1)
            plt.plot(mlp.loss_history, color=colors[i], label=name, linewidth=2)
            
            plt.subplot(2, 2, 2)
            plt.plot(mlp.accuracy_history, color=colors[i], label=name, linewidth=2)
            
            final_accuracy = mlp.accuracy_history[-1]
            
        else:  # MLP d√©taill√© pour architectures complexes
            mlp = MLPDetailed(list(arch), learning_rate=0.5)
            
            losses = []
            accuracies = []
            
            for epoch in range(1500):
                loss, pred = mlp.train_step(X_xor_expanded, y_xor_expanded)
                losses.append(loss)
                accuracy = np.mean((pred > 0.5) == y_xor_expanded)
                accuracies.append(accuracy)
            
            plt.subplot(2, 2, 1)
            plt.plot(losses, color=colors[i], label=name, linewidth=2)
            
            plt.subplot(2, 2, 2)
            plt.plot(accuracies, color=colors[i], label=name, linewidth=2)
            
            final_accuracy = accuracies[-1]
        
        # Calcul du nombre de param√®tres
        if len(arch) == 3:
            n_params = arch[0] * arch[1] + arch[1] + arch[1] * arch[2] + arch[2]
        else:
            n_params = sum(arch[j] * arch[j+1] + arch[j+1] for j in range(len(arch)-1))
        
        results.append((name, final_accuracy, n_params))
        
        plt.subplot(2, 2, 3)
        plt.bar(i, final_accuracy, color=colors[i], alpha=0.7)
        
        plt.subplot(2, 2, 4)
        plt.bar(i, n_params, color=colors[i], alpha=0.7)
    
    plt.subplot(2, 2, 1)
    plt.title('√âvolution de la perte par architecture')
    plt.xlabel('√âpoque')
    plt.ylabel('Perte')
    plt.legend()
    plt.grid(True)
    plt.yscale('log')
    
    plt.subplot(2, 2, 2)
    plt.title('√âvolution de la pr√©cision par architecture')
    plt.xlabel('√âpoque')
    plt.ylabel('Pr√©cision')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(2, 2, 3)
    plt.title('Pr√©cision finale par architecture')
    plt.ylabel('Pr√©cision')
    plt.xticks(range(len(names)), [n.split()[0] for n in names], rotation=45)
    plt.grid(True)
    
    plt.subplot(2, 2, 4)
    plt.title('Nombre de param√®tres')
    plt.ylabel('Param√®tres')
    plt.xticks(range(len(names)), [n.split()[0] for n in names], rotation=45)
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    # Tableau r√©capitulatif
    print("=== COMPARAISON ARCHITECTURES ===")
    print("Architecture | Pr√©cision | Param√®tres | Efficacit√©")
    print("-" * 55)
    for name, acc, params in results:
        efficiency = acc / params * 1000  # Pr√©cision par 1000 param√®tres
        print(f"{name:12} | {acc:8.3f} | {params:10d} | {efficiency:8.2f}")

compare_architectures()
```

---

## üéØ R√©capitulatif

**Points cl√©s √† retenir :**

### Neurone artificiel
- **Mod√®le** : Combinaison lin√©aire + fonction d'activation
- **Fonctions d'activation** : Sigmoid, tanh, ReLU, Leaky ReLU
- **Limitation** : S√©paration lin√©aire uniquement

### Perceptron
- **Algorithme historique** : Premier apprentissage supervis√©
- **R√®gle d'apprentissage** : Correction d'erreur simple
- **Limitation** : Probl√®mes non lin√©airement s√©parables (XOR)

### MLP (Multi-Layer Perceptron)
- **Architecture** : Couches de neurones connect√©es
- **Capacit√©** : Approximation universelle
- **Entra√Ænement** : R√©tropropagation du gradient

### R√©tropropagation
- **Principe** : Calcul efficace des gradients
- **√âtapes** : Forward ‚Üí Loss ‚Üí Backward ‚Üí Update
- **R√®gle de cha√Æne** : D√©rivation compos√©e

### Hyperparam√®tres
- **Learning rate** : Vitesse de convergence vs stabilit√©
- **Architecture** : Profondeur vs largeur
- **Activation** : Non-lin√©arit√© et gradient

### Applications
- **Classification** : Probl√®mes non lin√©aires
- **R√©gression** : Approximation de fonctions
- **Preprocessing** : Extraction de features

---

## üîó Pour aller plus loin

- **Optimisation** : SGD, Adam, RMSprop
- **R√©gularisation** : Dropout, batch normalization
- **CNN** : R√©seaux convolutionnels pour images
- **RNN** : R√©seaux r√©currents pour s√©quences