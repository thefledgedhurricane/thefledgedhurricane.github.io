'use client';

import React from 'react';
import CourseOverview from '@/components/lms/CourseOverview';
import { 
  Section, 
  Paragraph, 
  BulletList, 
  CodeBlock, 
  Callout, 
  SubSection, 
  ExampleBox,
  Highlight,
  InlineCode 
} from '@/components/lesson/LessonContent';

const lessons = [
  { 
    id: 1, 
    title: "R√©gression Lin√©aire: Fondements Math√©matiques", 
    duration: "75 min", 
    type: 'text' as const,
    exercises: [
      {
        id: 'ex1-1',
        title: 'Calculer les param√®tres',
        description: 'Donn√©es: [(1,2), (2,4), (3,5), (4,4), (5,5)]. Calculez w et b pour y = wx + b par la m√©thode des moindres carr√©s.',
        solution: 'Formule: w = (n‚àëxy - ‚àëx‚àëy) / (n‚àëx¬≤ - (‚àëx)¬≤)\nCalcul: w ‚âà 0.8, b ‚âà 1.6'
      }
    ],
    quiz: [
      {
        id: 'q1-1',
        question: 'Pourquoi utilise-t-on MSE (Mean Squared Error)?',
        options: [
          'C\'est plus facile √† calculer',
          '√áa p√©nalise plus fortement les grandes erreurs',
          'C\'est diff√©rentiable partout',
          'Les r√©ponses B et C'
        ],
        correctAnswer: 3,
        explanation: 'MSE = moyenne des carr√©s des erreurs. L\'√©l√©vation au carr√© (1) p√©nalise exponentiellement les grandes erreurs et (2) rend la fonction d√©rivable, essentiel pour la descente de gradient.'
      }
    ],
    cheatSheet: `üìö R√âGRESSION LIN√âAIRE

Mod√®le: $y = wx + b$
Loss (MSE): $L = \\frac{1}{n}\\sum(y_i - \\hat{y}_i)^2$
Gradient: $\\frac{\\partial L}{\\partial w} = \\frac{-2}{n}\\sum x_i(y_i - \\hat{y}_i)$

Update: $w := w - \\alpha \\cdot \\nabla L$

üí° Hypoth√®ses: Lin√©arit√©, Ind√©pendance, Normalit√© des r√©sidus`,
    content: {
      component: () => (
        <>
          <Section title="R√©gression Lin√©aire: Le Fondement de Tout">
            <Paragraph>
              La r√©gression lin√©aire est l'algorithme le plus simple et le plus important du Machine Learning. 
              Comprendre ses math√©matiques est essentiel pour tout le reste.
            </Paragraph>

            <SubSection title="Le Mod√®le Math√©matique">
              <div className="my-6 p-6 bg-gradient-to-r from-blue-50 to-purple-50 rounded-xl">
                <h5 className="font-semibold text-xl mb-4">Forme G√©n√©rale</h5>
                <div className="text-center text-3xl my-6">
                  <code>$\hat{`y`} = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n$</code>
                </div>
                <Paragraph>
                  Ou en notation vectorielle:
                </Paragraph>
                <div className="text-center text-3xl my-6">
                  <code>$\hat{`y`} = \mathbf{`w`}^T \mathbf{`x`} + b$</code>
                </div>
                <BulletList items={[
                  '$\\hat{y}$: Pr√©diction (valeur estim√©e)',
                  '$\\mathbf{x}$: Vecteur des features (variables d\'entr√©e)',
                  '$\\mathbf{w}$: Vecteur des poids (coefficients)',
                  '$b$: Biais (intercept)'
                ]} />
              </div>

              <ExampleBox title="Exemple: Pr√©dire le Prix d'une Maison">
                <Paragraph>
                  Variables: surface (m¬≤), nombre de chambres, √¢ge (ann√©es)
                </Paragraph>
                <CodeBlock language="python" code={`import numpy as np

# Donn√©es d'une maison
x = np.array([120, 3, 5])  # [surface, chambres, √¢ge]

# Poids appris (en euros)
w = np.array([2500, 15000, -5000])  # Impact de chaque variable
b = 50000  # Prix de base

# Pr√©diction
prix = np.dot(w, x) + b
# = 2500*120 + 15000*3 + (-5000)*5 + 50000
# = 300,000 + 45,000 - 25,000 + 50,000
# = 370,000 euros

print(f"Prix estim√©: {prix:,.0f} ‚Ç¨")`} />
              </ExampleBox>
            </SubSection>

            <SubSection title="Fonction de Co√ªt: Mean Squared Error (MSE)">
              <Paragraph>
                Comment mesurer la qualit√© de notre mod√®le? On compare les pr√©dictions aux vraies valeurs.
              </Paragraph>
              
              <div className="my-6 p-6 bg-white border-2 border-mckinsey-teal-500 rounded-xl">
                <h5 className="font-semibold text-lg mb-3">Formule MSE</h5>
                <div className="text-center text-2xl my-4">
                  <code>$L(\mathbf{`w`}, b) = \frac{`1`}{`n`} \sum_{`i=1`}^{`n`} (y_i - \hat{`y`}_i)^2$</code>
                </div>
                <div className="text-center text-2xl my-4">
                  <code>$= \frac{`1`}{`n`} \sum_{`i=1`}^{`n`} (y_i - (\mathbf{`w`}^T \mathbf{`x`}_i + b))^2$</code>
                </div>
                <Paragraph>
                  Objectif: <strong>Minimiser $L$</strong> en trouvant les meilleurs $\mathbf{`w`}$ et $b$.
                </Paragraph>
              </div>

              <Callout type="info">
                <Paragraph>
                  <strong>Pourquoi l'√©l√©vation au carr√©?</strong><br/>
                  1. Les erreurs n√©gatives ne s'annulent pas avec les positives<br/>
                  2. P√©nalise exponentiellement les grandes erreurs (erreur de 10 p√®se 100x plus qu'erreur de 1)<br/>
                  3. Fonction d√©rivable partout (contrairement √† |x|)
                </Paragraph>
              </Callout>

              <ExampleBox title="Calcul de MSE - Exemple Num√©rique">
                <CodeBlock language="python" code={`import numpy as np

# Donn√©es r√©elles
y_true = np.array([100, 150, 200, 250, 300])  # Prix r√©els (en k‚Ç¨)

# Pr√©dictions de notre mod√®le
y_pred = np.array([110, 140, 210, 240, 310])  # Prix pr√©dits

# Erreurs
erreurs = y_true - y_pred
# = [-10, 10, -10, 10, -10]

# Erreurs au carr√©
erreurs_carrees = erreurs ** 2
# = [100, 100, 100, 100, 100]

# MSE (moyenne)
mse = np.mean(erreurs_carrees)
# = 100 k‚Ç¨¬≤

# RMSE (Root MSE - plus interpr√©table)
rmse = np.sqrt(mse)
# = 10 k‚Ç¨

print(f"MSE: {mse}")
print(f"RMSE: {rmse} k‚Ç¨ d'erreur moyenne")`} />
              </ExampleBox>
            </SubSection>

            <SubSection title="Descente de Gradient: Trouver l'Optimum">
              <Paragraph>
                Comment trouver les poids qui minimisent la loss? <strong>Descente de gradient</strong>!
              </Paragraph>

              <div className="my-6 p-8 bg-gradient-to-br from-purple-50 via-blue-50 to-teal-50 rounded-2xl">
                <h5 className="font-bold text-2xl mb-6 text-center">Algorithme de Descente de Gradient</h5>
                
                <div className="space-y-6">
                  <div className="bg-white p-6 rounded-xl shadow-sm">
                    <p className="font-semibold text-lg mb-3">1. Calculer le Gradient</p>
                    <div className="text-center text-xl">
                      <code>$\frac{`\partial L`}{`\partial w_j`} = \frac{`-2`}{`n`} \sum_{`i=1`}^{`n`} x_{`ij`}(y_i - \hat{`y`}_i)$</code>
                    </div>
                    <div className="text-center text-xl mt-2">
                      <code>$\frac{`\partial L`}{`\partial b`} = \frac{`-2`}{`n`} \sum_{`i=1`}^{`n`} (y_i - \hat{`y`}_i)$</code>
                    </div>
                  </div>

                  <div className="bg-white p-6 rounded-xl shadow-sm">
                    <p className="font-semibold text-lg mb-3">2. Mise √† Jour des Param√®tres</p>
                    <div className="text-center text-xl">
                      <code>$w_j := w_j - \alpha \cdot \frac{`\partial L`}{`\partial w_j`}$</code>
                    </div>
                    <div className="text-center text-xl mt-2">
                      <code>$b := b - \alpha \cdot \frac{`\partial L`}{`\partial b`}$</code>
                    </div>
                    <Paragraph>
                      $\alpha$ = learning rate (taille du pas, ex: 0.01)
                    </Paragraph>
                  </div>

                  <div className="bg-white p-6 rounded-xl shadow-sm">
                    <p className="font-semibold text-lg mb-3">3. R√©p√©ter jusqu'√† Convergence</p>
                    <Paragraph>
                      Crit√®res d'arr√™t:<br/>
                      - Nombre d'it√©rations max (epochs)<br/>
                      - Loss ne diminue plus (‚àÜL {'<'} seuil)<br/>
                      - Gradients proches de 0
                    </Paragraph>
                  </div>
                </div>
              </div>

              <ExampleBox title="Impl√©mentation Compl√®te">
                <CodeBlock language="python" code={`import numpy as np
import matplotlib.pyplot as plt

# G√©n√©ration de donn√©es synth√©tiques
np.random.seed(42)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + bruit

# Ajout d'une colonne de 1 pour le biais
X_b = np.c_[np.ones((100, 1)), X]  # [1, x]

# Initialisation des param√®tres
theta = np.random.randn(2, 1)  # [b, w]
alpha = 0.1  # learning rate
n_iterations = 1000

# Descente de gradient
history_loss = []
for iteration in range(n_iterations):
    # Pr√©dictions
    y_pred = X_b.dot(theta)
    
    # Erreurs
    errors = y_pred - y
    
    # Calcul de la loss (MSE)
    mse = (errors ** 2).mean()
    history_loss.append(mse)
    
    # Calcul du gradient
    gradients = 2/100 * X_b.T.dot(errors)
    
    # Mise √† jour
    theta = theta - alpha * gradients
    
    if iteration % 100 == 0:
        print(f"Iteration {iteration}: MSE = {mse:.4f}")

print(f"\\nParam√®tres finaux:")
print(f"b = {theta[0][0]:.2f}")  # ‚âà 4
print(f"w = {theta[1][0]:.2f}")  # ‚âà 3

# Visualisation
plt.figure(figsize=(12, 4))

# Graphique 1: Donn√©es et mod√®le
plt.subplot(1, 2, 1)
plt.scatter(X, y, alpha=0.5)
plt.plot(X, X_b.dot(theta), 'r-', linewidth=2, label='Mod√®le appris')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.title('R√©gression Lin√©aire')

# Graphique 2: Convergence
plt.subplot(1, 2, 2)
plt.plot(history_loss)
plt.xlabel('It√©ration')
plt.ylabel('MSE')
plt.title('Convergence de la Loss')
plt.yscale('log')
plt.grid(True)

plt.tight_layout()
plt.show()`} />
              </ExampleBox>
            </SubSection>

            <SubSection title="Solution Analytique: Normal Equation">
              <Paragraph>
                Il existe une formule ferm√©e pour la r√©gression lin√©aire (pas besoin de gradient!):
              </Paragraph>
              
              <div className="my-6 p-6 bg-amber-50 border-2 border-amber-500 rounded-xl">
                <h5 className="font-bold text-xl mb-4 text-center">√âquation Normale</h5>
                <div className="text-center text-2xl my-6">
                  <code>$\mathbf{`w`} = (\mathbf{`X`}^T \mathbf{`X`})^{`-1`} \mathbf{`X`}^T \mathbf{`y`}$</code>
                </div>
                <Paragraph>
                  Solution optimale en une seule √©tape (pas d'it√©rations)!
                </Paragraph>
              </div>

              <div className="grid md:grid-cols-2 gap-6 my-6">
                <div className="bg-green-50 border-2 border-green-500 rounded-xl p-6">
                  <h6 className="font-bold mb-3">‚úÖ Avantages</h6>
                  <BulletList items={[
                    'Pas de learning rate √† choisir',
                    'Pas d\'it√©rations',
                    'Solution exacte garantie'
                  ]} />
                </div>

                <div className="bg-red-50 border-2 border-red-500 rounded-xl p-6">
                  <h6 className="font-bold mb-3">‚ùå Inconv√©nients</h6>
                  <BulletList items={[
                    'Co√ªteux: O(n¬≥) pour inverser la matrice',
                    'Impossible si X^T X non inversible',
                    'Pas adapt√© aux gros datasets (>10,000 features)'
                  ]} />
                </div>
              </div>

              <CodeBlock language="python" code={`import numpy as np

# M√©thode 1: Descente de gradient (it√©rative)
def gradient_descent(X, y, alpha=0.01, n_iter=1000):
    theta = np.random.randn(X.shape[1], 1)
    for _ in range(n_iter):
        gradients = 2/len(X) * X.T.dot(X.dot(theta) - y)
        theta = theta - alpha * gradients
    return theta

# M√©thode 2: √âquation normale (directe)
def normal_equation(X, y):
    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)

# Comparaison
X_b = np.c_[np.ones((100, 1)), X]
theta_gd = gradient_descent(X_b, y)
theta_ne = normal_equation(X_b, y)

print("Gradient Descent:", theta_gd.T)
print("Normal Equation: ", theta_ne.T)
# Les deux donnent le m√™me r√©sultat!`} />
            </SubSection>

            <SubSection title="R√©gression Polynomiale: Non-Lin√©arit√©">
              <Paragraph>
                La r√©gression "lin√©aire" peut mod√©liser des relations non-lin√©aires!
              </Paragraph>

              <ExampleBox title="Transformer les Features">
                <Paragraph>
                  Au lieu de $y = w_1 x + b$, on peut faire:
                </Paragraph>
                <div className="text-center text-xl my-4">
                  <code>$y = w_1 x + w_2 x^2 + w_3 x^3 + b$</code>
                </div>
                <Paragraph>
                  C'est toujours lin√©aire... <strong>en les param√®tres $w$</strong>!
                </Paragraph>

                <CodeBlock language="python" code={`from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np

# Donn√©es non-lin√©aires (parabole)
X = np.linspace(-3, 3, 100).reshape(-1, 1)
y = 0.5 * X**2 + X + 2 + np.random.randn(100, 1) * 0.5

# Cr√©er features polynomiales: [x, x¬≤, x¬≥]
poly_features = PolynomialFeatures(degree=3, include_bias=False)
X_poly = poly_features.fit_transform(X)

# R√©gression lin√©aire sur features transform√©es
model = LinearRegression()
model.fit(X_poly, y)

# Pr√©diction
y_pred = model.predict(X_poly)

print(f"Coefficients: {model.coef_}")
print(f"Intercept: {model.intercept_}")`} />
              </ExampleBox>

              <Callout type="warning">
                <Paragraph>
                  <strong>Attention √† l'overfitting!</strong> Un polyn√¥me de degr√© √©lev√© peut parfaitement 
                  fitter les donn√©es d'entra√Ænement mais g√©n√©raliser tr√®s mal. Solution: <Highlight>R√©gularisation</Highlight>.
                </Paragraph>
              </Callout>
            </SubSection>

            <SubSection title="R√©gularisation: Ridge et Lasso">
              <Paragraph>
                Pour √©viter l'overfitting, on p√©nalise les poids trop grands.
              </Paragraph>

              <div className="grid md:grid-cols-2 gap-6 my-6">
                <div className="bg-blue-50 border-2 border-blue-500 rounded-xl p-6">
                  <h5 className="font-bold text-lg mb-3">Ridge (L2)</h5>
                  <div className="text-center text-lg my-4">
                    <code>$L = MSE + \lambda \sum w_j^2$</code>
                  </div>
                  <BulletList items={[
                    'P√©nalise le carr√© des poids',
                    'R√©duit l\'amplitude des poids',
                    'Tous les features conserv√©s'
                  ]} />
                </div>

                <div className="bg-purple-50 border-2 border-purple-500 rounded-xl p-6">
                  <h5 className="font-bold text-lg mb-3">Lasso (L1)</h5>
                  <div className="text-center text-lg my-4">
                    <code>$L = MSE + \lambda \sum |w_j|$</code>
                  </div>
                  <BulletList items={[
                    'P√©nalise la valeur absolue',
                    'Peut mettre certains poids √† 0',
                    'S√©lection automatique de features'
                  ]} />
                </div>
              </div>

              <CodeBlock language="python" code={`from sklearn.linear_model import Ridge, Lasso

# Ridge Regression
ridge = Ridge(alpha=1.0)  # Œª = 1
ridge.fit(X_train, y_train)

# Lasso Regression
lasso = Lasso(alpha=0.1)  # Œª = 0.1
lasso.fit(X_train, y_train)

# Comparer les coefficients
print("Ridge coefficients:", ridge.coef_)
print("Lasso coefficients:", lasso.coef_)
# Lasso aura des 0 (features √©limin√©es)`} />
            </SubSection>
          </Section>
        </>
      )
    },
    details: "Math√©matiques compl√®tes de la r√©gression lin√©aire: MSE, gradient descent, √©quation normale, r√©gularisation."
  }
];

const objectives = [
  "Ma√Ætriser les math√©matiques de la r√©gression lin√©aire",
  "Impl√©menter la descente de gradient from scratch",
  "Comprendre MSE, RMSE, R¬≤",
  "Utiliser la r√©gularisation (Ridge, Lasso)",
  "Appliquer √† des probl√®mes r√©els"
];

const prerequisites = [
  "Alg√®bre lin√©aire (matrices, produit matriciel)",
  "Calcul diff√©rentiel (d√©riv√©es, gradient)",
  "Python et NumPy",
  "Module 'Introduction √† l'IA' compl√©t√©"
];

export default function IAApprentissageSupervise() {
  return (
    <CourseOverview
      title="Apprentissage Supervis√©: R√©gression"
      description="Math√©matiques approfondies de la r√©gression lin√©aire et non-lin√©aire"
      level="Interm√©diaire"
      duration="12h"
      lessonCount={1}
      lessons={lessons}
      objectives={objectives}
      prerequisites={prerequisites}
    />
  );
}
